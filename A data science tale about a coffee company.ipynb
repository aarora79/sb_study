{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data science project for ANLY-501 | A data science tale about a coffee company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This notebook provides a project report for the data science project done as part of ANLY-501. The data science project intends to show all stages of the data science pipeline. This notebook facilitates that by organizing different phases into different sections and having the code and visualizations all included in one place. Install Jupyter notebook software available from http://jupyter.org/ to run this notebook.\n",
    ">All code for this project is available on Github as https://github.com/aarora79/sb_study and there is also a simple website associated with it https://aarora79.github.io/sb_study/. The entire code (including the generated output) can be downloaded as a zip file from Github via this URL https://github.com/aarora79/sb_study/archive/master.zip. To run the code, simply unzip the master.zip and say \"python main.py\". The code requires that Python 3.5.2|Anaconda 4.1.1 be installed on the machine. Also, since the code tries to get the datasets in realtime so an Internet connection is also required.\n",
    "\n",
    "<span style=\"color:blue\">[<b>Updated for Project2 11/7/2016, starting from section <i>Combinining the two datasets</i></b>]</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starbucks Corporation is an American coffee company and coffeehouse chain with more than 24,000 stores across the world. This projects intends to explore the following data science problems:\n",
    "1. Exploratory Data Analysis (EDA) about Starbucks store locations for example geographical distribution of stores by country, region, ownership model, brand name etc.\n",
    "2. Find a relationship between Starbucks data with various economic and human development indices such as GDP, ease of doing business, rural to urban population ratio, literacy rate, revenue from tourist inflow and so on.\n",
    "3. Predict which countries where Starbucks does not have a store today are most suitable for having Starbucks stores (in other words in which country where Starbucks does not have a presence should Starbucks open its next store and how many).\n",
    "\n",
    "This problem is important because it attempts to provide a model (using Starbucks as an example) which can be applied to any similar business (say in the food and hospitality industry) to predict where it could expand globally. It provides insight about where the business is located currently by bringing out information like say the total number of stores (48) found in the entire continent of Africa is way less than number of stores (184) found just on U.S. Airports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Analysis that Can Be Conducted Using Collected Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used as part of this project is obtained from two sources.\n",
    "1. Starbucks store location data is available from https://opendata.socrata.com via an API. The Socrate Open Data API (SODA) end point for this data is https://opendata.socrata.com/resource/xy4y-c4mk.json\n",
    "\n",
    "2. The economic and urban development data for various countries is available from the World Bank(WB) website. WB APIs are described here https://datahelpdesk.worldbank.org/knowledgebase/topics/125589-developer-information. The API end point for all the World Development Indicators (WDI) is http://api.worldbank.org/indicators?format=json. Some examples of indicators being collected include GDP, urban to rural population ratio, % of employed youth (15-24 years), international tourism receipts (as % of total exports), ease of doing business and so on.\n",
    "\n",
    "The possible directions/hypothesis based on collected data (this is not the complete list, would be expanded as the project goes on):\n",
    "1. EDA about Starbucks store locations (for example):\n",
    " - What percentage of stores exists in high income, high literacy, high urban to rural population ratio European countries Vs say high population, rising GDP, low urban to rural population Asian countries.\n",
    " - Distribution of stores across geographies based on type of ownership (franchisee, joint venture etc.), brand name etc.\n",
    " - Which country, which city has the most Starbucks stores per 1000 people.\n",
    " - Is there a Starbucks always open at any UTC time during a 24hour period i.e. you can always find some Starbucks store open time at any given time somewhere in some timezone around the world.\n",
    "\n",
    "2. Data visualization of the Starbucks store data:\n",
    " - World map showing starbucks locations around the world.\n",
    " - Heat map of the world based on the number of Starbucks store in a country.\n",
    " - Frequency distribution of Starbucks store by city in a given country. Does this distribution resemble any wellll known statistical distribution.\n",
    " - Parallel coordinates based visualization for number of stores combined with economic and urban development indicators.\n",
    " \n",
    "3. Machine learning model for predicting number of Starbucks store based on various economic and urban development indicators. This could then be used to predict which countries where Starbucks does not have a presence today would be best suited as new markets. For example, model the number of Starbucks location in a country based on a) ease of doing business, b) GDP, c) urban to rural population, d) employment rate between people in the 15-24year age group, e) type of government, f) access to 24hour electricity and so on and so forth.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this project is being obtained via APIs from the Socrata web site and the World Bank website and is therefore expected to be relatively error free (for example as compared to the same data being obtained by scraping these websites). Even so, the data is checked for quality and appropriate error handling or even alternate mechanisms are put in place to handle errors/issues with the data.\n",
    "\n",
    "| Issue         | Handling Strategy| \n",
    "| ------------- |-------------| \n",
    "| Some of the city names (for examples cities in China) include UTF-16 characters and would therefore not display correctly in documents and charts.      |  Replace city name with country name _1, _2 and so on, for example CN_1, CN_2 etc.|\n",
    "| Missing data in any of the fields in the Starbucks dataset. | Ignore the data for the location with any mandatory field missing (say country name is missing). Keep a count of the locations ignored due to missing data to get a sense of the overall quality of data. |\n",
    "| Incorrect format of the value in various fields in the Starbucks dataset. For example Latitude/Longitude values being out of range, country codes being invalid etc.|  Ignore the data for the location with any missing value. Keep a count of the locations ignored due to missing data to get a sense of the overall quality of data. |\n",
    "| Misc. data type related errors such as date time field (first_seen field in Starbucks dataset) not being correct, fields considered as primary key not being unique (for example store id for Starbuck dataset, Country code for WB dataset) | Flag all invalid fields as errors. |\n",
    "| Missing data for any of the indicators in the WB dataset. | The most recent year for which the data is available is 2015, if for a particular indicator the 2015 data is not available then use data for the previous year i.e. 2014. If no data is available for that indicator even for the previous 5 years then flag it as such and have the user define some custom value for it.|\n",
    "| Incorrect format of the value in various fields in the WB dataset. For example alphanumeric or non-numeric data  for fields such as GDP for which numeric values are expected.|  Provide sufficient information to the user (in this case the programmer) about the incorrect data and have the user define correct values.|\n",
    "\n",
    "The subsequent sections of this notebook provide a view of the data for the two datasets used in this project and also provide the data quality scores (DQS) for both the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets (Starbucks and WorldBank WDI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python code for the SB Study (SBS) is run offline and the results (along with the code) are periodically pushed to Github. The code here simply downloads the files from Github to show the results and how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starbucks dataset has 24823 rows and 21 columns \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>city</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>country</th>\n",
       "      <th>country_subdivision</th>\n",
       "      <th>current_timezone_offset</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>ownership_type</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_number</th>\n",
       "      <th>street_1</th>\n",
       "      <th>street_2</th>\n",
       "      <th>street_3</th>\n",
       "      <th>street_combined</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>{u'latitude': u'22.3407001495361', u'needs_rec...</td>\n",
       "      <td>CN</td>\n",
       "      <td>91</td>\n",
       "      <td>480</td>\n",
       "      <td>2013-12-08T22:41:59</td>\n",
       "      <td>22.340700</td>\n",
       "      <td>114.201691</td>\n",
       "      <td>Plaza Hollywood</td>\n",
       "      <td>...</td>\n",
       "      <td>LS</td>\n",
       "      <td>{u'phone_number': u'29554570'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>34638-85784</td>\n",
       "      <td>Level 2, Plaza Hollywood, Diamond Hill,</td>\n",
       "      <td>Kowloon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Level 2, Plaza Hollywood, Diamond Hill,, Kowloon</td>\n",
       "      <td>China Standard Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>{u'latitude': u'22.2839393615723', u'needs_rec...</td>\n",
       "      <td>CN</td>\n",
       "      <td>91</td>\n",
       "      <td>480</td>\n",
       "      <td>2013-12-08T22:41:59</td>\n",
       "      <td>22.283939</td>\n",
       "      <td>114.158188</td>\n",
       "      <td>Exchange Square</td>\n",
       "      <td>...</td>\n",
       "      <td>LS</td>\n",
       "      <td>{u'phone_number': u'21473739'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>34601-20281</td>\n",
       "      <td>Shops 308-310, 3/F.,</td>\n",
       "      <td>Exchange Square Podium, Central, HK.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shops 308-310, 3/F.,, Exchange Square Podium, ...</td>\n",
       "      <td>China Standard Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Kowloon</td>\n",
       "      <td>{u'latitude': u'22.3228702545166', u'needs_rec...</td>\n",
       "      <td>CN</td>\n",
       "      <td>91</td>\n",
       "      <td>480</td>\n",
       "      <td>2013-12-08T22:41:59</td>\n",
       "      <td>22.322870</td>\n",
       "      <td>114.213440</td>\n",
       "      <td>Telford Plaza</td>\n",
       "      <td>...</td>\n",
       "      <td>LS</td>\n",
       "      <td>{u'phone_number': u'27541323'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>34610-28207</td>\n",
       "      <td>Shop Unit G1A, Atrium A, Telford Plaza I</td>\n",
       "      <td>, Kowloon Bay, Kowloon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shop Unit G1A, Atrium A, Telford Plaza I, , Ko...</td>\n",
       "      <td>China Standard Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>{u'latitude': u'22.2844505310059', u'needs_rec...</td>\n",
       "      <td>CN</td>\n",
       "      <td>91</td>\n",
       "      <td>480</td>\n",
       "      <td>2013-12-08T22:41:59</td>\n",
       "      <td>22.284451</td>\n",
       "      <td>114.158463</td>\n",
       "      <td>Hong Kong Station</td>\n",
       "      <td>...</td>\n",
       "      <td>LS</td>\n",
       "      <td>{u'phone_number': u'25375216'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>34622-64463</td>\n",
       "      <td>Concession HOK 3a &amp; b</td>\n",
       "      <td>LAR Hong Kong Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concession HOK 3a &amp; b, LAR Hong Kong Station</td>\n",
       "      <td>China Standard Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>{u'latitude': u'22.2777309417725', u'needs_rec...</td>\n",
       "      <td>CN</td>\n",
       "      <td>91</td>\n",
       "      <td>480</td>\n",
       "      <td>2013-12-08T22:41:59</td>\n",
       "      <td>22.277731</td>\n",
       "      <td>114.164917</td>\n",
       "      <td>Pacific Place, Central</td>\n",
       "      <td>...</td>\n",
       "      <td>LS</td>\n",
       "      <td>{u'phone_number': u'29184762'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>34609-22927</td>\n",
       "      <td>Shop 131, Level 1, Pacific Place</td>\n",
       "      <td>88 Queensway, HK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shop 131, Level 1, Pacific Place, 88 Queensway...</td>\n",
       "      <td>China Standard Time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       brand       city                                        coordinates  \\\n",
       "0  Starbucks  Hong Kong  {u'latitude': u'22.3407001495361', u'needs_rec...   \n",
       "1  Starbucks  Hong Kong  {u'latitude': u'22.2839393615723', u'needs_rec...   \n",
       "2  Starbucks    Kowloon  {u'latitude': u'22.3228702545166', u'needs_rec...   \n",
       "3  Starbucks  Hong Kong  {u'latitude': u'22.2844505310059', u'needs_rec...   \n",
       "4  Starbucks  Hong Kong  {u'latitude': u'22.2777309417725', u'needs_rec...   \n",
       "\n",
       "  country country_subdivision  current_timezone_offset           first_seen  \\\n",
       "0      CN                  91                      480  2013-12-08T22:41:59   \n",
       "1      CN                  91                      480  2013-12-08T22:41:59   \n",
       "2      CN                  91                      480  2013-12-08T22:41:59   \n",
       "3      CN                  91                      480  2013-12-08T22:41:59   \n",
       "4      CN                  91                      480  2013-12-08T22:41:59   \n",
       "\n",
       "    latitude   longitude                    name         ...           \\\n",
       "0  22.340700  114.201691         Plaza Hollywood         ...            \n",
       "1  22.283939  114.158188         Exchange Square         ...            \n",
       "2  22.322870  114.213440           Telford Plaza         ...            \n",
       "3  22.284451  114.158463       Hong Kong Station         ...            \n",
       "4  22.277731  114.164917  Pacific Place, Central         ...            \n",
       "\n",
       "  ownership_type                    phone_number postal_code store_id  \\\n",
       "0             LS  {u'phone_number': u'29554570'}         NaN        1   \n",
       "1             LS  {u'phone_number': u'21473739'}         NaN        6   \n",
       "2             LS  {u'phone_number': u'27541323'}         NaN        8   \n",
       "3             LS  {u'phone_number': u'25375216'}         NaN       13   \n",
       "4             LS  {u'phone_number': u'29184762'}         NaN       17   \n",
       "\n",
       "   store_number                                  street_1  \\\n",
       "0   34638-85784   Level 2, Plaza Hollywood, Diamond Hill,   \n",
       "1   34601-20281                      Shops 308-310, 3/F.,   \n",
       "2   34610-28207  Shop Unit G1A, Atrium A, Telford Plaza I   \n",
       "3   34622-64463                     Concession HOK 3a & b   \n",
       "4   34609-22927          Shop 131, Level 1, Pacific Place   \n",
       "\n",
       "                               street_2 street_3  \\\n",
       "0                               Kowloon      NaN   \n",
       "1  Exchange Square Podium, Central, HK.      NaN   \n",
       "2                , Kowloon Bay, Kowloon      NaN   \n",
       "3                 LAR Hong Kong Station      NaN   \n",
       "4                      88 Queensway, HK      NaN   \n",
       "\n",
       "                                     street_combined             timezone  \n",
       "0   Level 2, Plaza Hollywood, Diamond Hill,, Kowloon  China Standard Time  \n",
       "1  Shops 308-310, 3/F.,, Exchange Square Podium, ...  China Standard Time  \n",
       "2  Shop Unit G1A, Atrium A, Telford Plaza I, , Ko...  China Standard Time  \n",
       "3       Concession HOK 3a & b, LAR Hong Kong Station  China Standard Time  \n",
       "4  Shop 131, Level 1, Pacific Place, 88 Queensway...  China Standard Time  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#get the Starbucks dataset from Github\n",
    "#note that the dataset \n",
    "SB_DATASET_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/SB_data_as_downloaded.csv'\n",
    "df = pd.read_csv(SB_DATASET_URL)\n",
    "\n",
    "print('Starbucks dataset has %d rows and %d columns ' %(df.shape[0], df.shape[1]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worldbank dataset has 264 rows and 84 columns \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>IC.TAX.LABR.CP.ZS</th>\n",
       "      <th>WP_time_01.1</th>\n",
       "      <th>SP.POP.1564.TO.ZS</th>\n",
       "      <th>IC.BUS.NDNS.ZS</th>\n",
       "      <th>IC.LGL.CRED.XQ</th>\n",
       "      <th>IC.GOV.DURS.ZS</th>\n",
       "      <th>DT.DOD.PVLX.GN.ZS</th>\n",
       "      <th>SE.ADT.LITR.ZS</th>\n",
       "      <th>IC.EXP.COST.CD</th>\n",
       "      <th>...</th>\n",
       "      <th>SL.SRV.EMPL.ZS</th>\n",
       "      <th>FI.RES.TOTL.DT.ZS</th>\n",
       "      <th>IC.FRM.BRIB.ZS</th>\n",
       "      <th>IC.TAX.OTHR.CP.ZS</th>\n",
       "      <th>IC.REG.COST.PC.ZS</th>\n",
       "      <th>IC.ELC.OUTG</th>\n",
       "      <th>SL.EMP.WORK.ZS</th>\n",
       "      <th>TX.VAL.OTHR.ZS.WT</th>\n",
       "      <th>EN.URB.LCTY.UR.ZS</th>\n",
       "      <th>SI.POV.2DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.579469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.451180</td>\n",
       "      <td>31.889816</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BE</td>\n",
       "      <td>49.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.830742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.897278</td>\n",
       "      <td>18.516810</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BF</td>\n",
       "      <td>21.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.034628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7</td>\n",
       "      <td>43.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.703959</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BG</td>\n",
       "      <td>20.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.828506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.100215</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VE</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.627593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.1</td>\n",
       "      <td>88.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.755198</td>\n",
       "      <td>10.534170</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code  IC.TAX.LABR.CP.ZS  WP_time_01.1  SP.POP.1564.TO.ZS  \\\n",
       "0           BD                NaN           NaN          65.579469   \n",
       "1           BE               49.4           NaN          64.830742   \n",
       "2           BF               21.4           NaN          52.034628   \n",
       "3           BG               20.2           NaN          65.828506   \n",
       "4           VE               18.0           NaN          65.627593   \n",
       "\n",
       "   IC.BUS.NDNS.ZS  IC.LGL.CRED.XQ  IC.GOV.DURS.ZS  DT.DOD.PVLX.GN.ZS  \\\n",
       "0             NaN             6.0             NaN                NaN   \n",
       "1             NaN             4.0             NaN                NaN   \n",
       "2             NaN             6.0             NaN                NaN   \n",
       "3             NaN             9.0             NaN                NaN   \n",
       "4             NaN             1.0             NaN                NaN   \n",
       "\n",
       "   SE.ADT.LITR.ZS  IC.EXP.COST.CD     ...       SL.SRV.EMPL.ZS  \\\n",
       "0             NaN             NaN     ...                  NaN   \n",
       "1             NaN             NaN     ...                  NaN   \n",
       "2             NaN             NaN     ...                  NaN   \n",
       "3             NaN             NaN     ...                  NaN   \n",
       "4             NaN             NaN     ...                  NaN   \n",
       "\n",
       "   FI.RES.TOTL.DT.ZS  IC.FRM.BRIB.ZS  IC.TAX.OTHR.CP.ZS  IC.REG.COST.PC.ZS  \\\n",
       "0                NaN             NaN                NaN               13.9   \n",
       "1                NaN             NaN                0.6                4.8   \n",
       "2                NaN             NaN                3.7               43.5   \n",
       "3                NaN             NaN                1.8                0.7   \n",
       "4                NaN             NaN               37.1               88.7   \n",
       "\n",
       "   IC.ELC.OUTG  SL.EMP.WORK.ZS  TX.VAL.OTHR.ZS.WT  EN.URB.LCTY.UR.ZS  \\\n",
       "0          NaN             NaN          62.451180          31.889816   \n",
       "1          NaN             NaN          59.897278          18.516810   \n",
       "2          NaN             NaN                NaN          50.703959   \n",
       "3          NaN             NaN                NaN          23.100215   \n",
       "4          NaN             NaN          14.755198          10.534170   \n",
       "\n",
       "   SI.POV.2DAY  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#get the Starbucks dataset from Github\n",
    "#note that the dataset \n",
    "WB_DATASET_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/WDI_data_as_downloaded.csv'\n",
    "df = pd.read_csv(WB_DATASET_URL)\n",
    "\n",
    "print('Worldbank dataset has %d rows and %d columns ' %(df.shape[0], df.shape[1]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Scores (DQS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for dataquality two types of checks were done on both the datasets.\n",
    "1. **Missing data**: any cell in the dataset which was empty was counted as missing data. This is checked for all features \n",
    "   in both the datasets.\n",
    "2. **Invalid data**: validation checks were done on many (9 fields in the Starbucks dataset, \n",
    "   and all 83 fields in the Worldbank dataset). These checks were both generic (validation for numeric data, \n",
    "   absence of special characters, validation of timestamp data, latitude and longitude validation, timezone offset and \n",
    "   so on and so forth) as well context specific (store-id has to be unique).\n",
    "\n",
    "Two metrices are derived to quantify missing data and invalid data. These are raw score and adjusted score.\n",
    "\n",
    "1. **Raw Score for Missing Data**: this is the percentage of data that is not missing, simply speaking this is the count of non-empty cells Vs total number of cells expressed as a percentage. The higher this score the cleaner the dataset is from the perspective of missing data. This score is available for both Starbucks and WorldBank datasets.\n",
    "\n",
    "2. **Adjusted Raw Score for Missing Data**: this is the percentage of data that is not missing for **Mandatory Features** (i.e. features without which the record would have to be discarded), simply speaking this is the count of non-empty cells in mandatory columns Vs total number of cells expressed as a percentage. In case all features are mandatory then the adjusted raw score and the raw score are the same. The higher this score the cleaner the dataset is from the perspective of missing data. This score is available for both Starbucks and WorldBank datasets. \n",
    "\n",
    "3. **Raw Score for Invalid Data**: this is the percentage of data that is not invalid, simply speaking this is the count of cells containing valid data Vs total number of cells expressed as a percentage. The higher this score the cleaner the dataset is from the perspective of invalid data. Validity checks are both generic and context specific as defined above. This score is available for both Starbucks and WorldBank datasets.\n",
    "\n",
    "3. **Adjusted Score for Invalid Data**: this is the percentage of data that is not invalid for **Mandatory Features** (i.e. features without which the record would have to be discarded), simply speaking this is the count of cells containing valid data in mandatory columns Vs total number of cells expressed as a percentage. The higher this score the cleaner the dataset is from the perspective of invalid data. Validity checks are both generic and context specific as defined above. This score is available for both Starbucks and WorldBank datasets.\n",
    "\n",
    "### Validity checks implemented for Starbucks dataset\n",
    "\n",
    "| Field         | Validity Check| \n",
    "| ------------- |-------------| \n",
    "| Store Id      |  Has to be unique for each row|\n",
    "| Latitude/Longitude|Longitude measurements range from 0° to (+/–)180°, Latitude from -90° to +90°|\n",
    "| Timezone offset| Has to be divisible by 15|\n",
    "| Country code | Has to be present in a country code list downloaded offline|\n",
    "| Brand | Has to be within one of 6 brands listed on Starbucks website http://www.starbucks.com/careers/brands|\n",
    "| Store Number | Has to follow a format XXXX-YYYY|\n",
    "| Timzone name| Has to have either \"Standard Time\" or  \"UTC\" as part of name|\n",
    "| Ownership Type | Should not contain special characters | \n",
    "| First seen | Has to follow a date format |\n",
    "\n",
    "### Validity checks implemented for Worldbank dataset\n",
    "| Field         | Validity Check| \n",
    "| ------------- |-------------| \n",
    "| Country Code      |  Should not contain special characters|\n",
    "| All 83 features (WDIs)| Should be numeric as all of them represent a numerical measure of the feature|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQS for Starbucks and WorldBank datasets\n",
    "The following code fragment downloads a CSV file containing these scores from the Github repo of this project. These scrores have been calculated offline by running the program and uploading the results as part of the Github repo. The missing data raw score for the WorldBank data is very less ~34%, this is because for several parameters the 2015 data is not yet available, so in this case we will use the 2014 data as these are macro level indicators which do not see a drastic change year on year (this will be disussed more in the next phase of the project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datasource</th>\n",
       "      <th>Invalid_Data_Raw_Score</th>\n",
       "      <th>Invalid_Data_Adjusted_Score</th>\n",
       "      <th>Missing_Data_Raw_Score</th>\n",
       "      <th>Missing_Data_Adjusted_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WorldBank</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33.648990</td>\n",
       "      <td>33.64899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Starbucks</td>\n",
       "      <td>99.999233</td>\n",
       "      <td>99.999233</td>\n",
       "      <td>91.523031</td>\n",
       "      <td>100.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Datasource  Invalid_Data_Raw_Score  Invalid_Data_Adjusted_Score  \\\n",
       "0  WorldBank              100.000000                   100.000000   \n",
       "1  Starbucks               99.999233                    99.999233   \n",
       "\n",
       "   Missing_Data_Raw_Score  Missing_Data_Adjusted_Score  \n",
       "0               33.648990                     33.64899  \n",
       "1               91.523031                    100.00000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DQS_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/dqs.csv'\n",
    "\n",
    "df = pd.read_csv(DQS_URL)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation\n",
    "Three (for now) new features are created using the datasets. These features are all added to the Starbucks dataset. These are described below:\n",
    "\n",
    "| Feature       | Format| Explanation         |\n",
    "| ------------- |-------------|---------------| \n",
    "| Continent     | String | Map the country code to the continent to which the country belongs. This feature enables answering basic questions like what % of stores exist in Asia as compared to North America for  example. The country code to continent mapping is obtained with the help of two csv files downloaded offline (code to country mapping, country to continent mapping), first country code is mapped to country name and then country name is mapped to the continent name.|\n",
    "|On Airport     | Boolean  | Does this store exists on an Airport? This feature helps answer questions like how many of the Starbucks in Europe exists on Airport for example. For now a store is considered to be on an airport if the name field or the street combined field for a store happens to have either one the following terms \"Airport, Arpt, Terminal\" contained in it. Going forward this logic will be replaced with an IATA API (see comments in code, wb.py file).|\n",
    "|Ease of doing business Category| String | The ease of doing business index (obtained from the WorldBank data) for the country in which the store exists is mapped to a category viz. Very High (1 to 10), High (11 to 30), Medium (31, 90), Low (91 to 130), Very Low (131 and beyond) and finally Unknown for which countries where there is no data available. This feature provides insights like around 70% of Starbucks stores exist in countries which have an ease of doing business index between 1 to 10 (with 1 being the highest and 189 being the lowest). The ease of doing business index is obtained by looking up the country code in the WDI dataset to find the value for the ease of doing business index.|\n",
    "\n",
    "The following table shows a view of the modified dataset with new features added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>country</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>brand</th>\n",
       "      <th>continent</th>\n",
       "      <th>on_airport</th>\n",
       "      <th>eodb_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CN</td>\n",
       "      <td>Plaza Hollywood</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Asia</td>\n",
       "      <td>False</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>CN</td>\n",
       "      <td>Exchange Square</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Asia</td>\n",
       "      <td>False</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>CN</td>\n",
       "      <td>Telford Plaza</td>\n",
       "      <td>Kowloon</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Asia</td>\n",
       "      <td>False</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>CN</td>\n",
       "      <td>Hong Kong Station</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Asia</td>\n",
       "      <td>False</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>CN</td>\n",
       "      <td>Pacific Place, Central</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>Asia</td>\n",
       "      <td>False</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id country                    name       city      brand continent  \\\n",
       "0         1      CN         Plaza Hollywood  Hong Kong  Starbucks      Asia   \n",
       "1         6      CN         Exchange Square  Hong Kong  Starbucks      Asia   \n",
       "2         8      CN           Telford Plaza    Kowloon  Starbucks      Asia   \n",
       "3        13      CN       Hong Kong Station  Hong Kong  Starbucks      Asia   \n",
       "4        17      CN  Pacific Place, Central  Hong Kong  Starbucks      Asia   \n",
       "\n",
       "  on_airport eodb_category  \n",
       "0      False             M  \n",
       "1      False             M  \n",
       "2      False             M  \n",
       "3      False             M  \n",
       "4      False             M  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "SB_W_NEW_FEATURES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/SB_data_w_features.csv'\n",
    "\n",
    "df = pd.read_csv(SB_W_NEW_FEATURES_URL)\n",
    "\n",
    "#show only some of the columns so that new feature can be shown without a scrollbar\n",
    "cols = ['store_id', 'country', 'name', 'city', 'brand', 'continent', 'on_airport', 'eodb_category']\n",
    "df[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some notes about the code and how to run the SBS program\n",
    "This section provides information about how to run the code, how is the code organized amongst different modules and what are the output files generated.\n",
    "\n",
    "## How to run the code\n",
    "When you unzip the zip file from Blackboard Github, you will see a folder called **sb_study** created which contains all the code, helper files and even an output folder (the contents of the output folder get rewritten on every run of the SBS program). As a pre-requisite the machine on which this program is being run is required to have Python 3.5.2|Anaconda 4.1.1 installed. Run the program by simply saying\n",
    "\n",
    "*python main.py*\n",
    "\n",
    "**The code will take about 2 to 3 minutes to run completely.** As the program runs it would spit out traces on the console and these traces are also automatically being logged into a file called SBS.log created in the output directory (also created by the SBS program in the current working directory if not already present). The program would create a directory called output and several sub-directories inside it to store the various artifacts created (csv files, plots etc.) as part of running the program.\n",
    "\n",
    "The program can also be run in analysis only mode such that it assumes that the data has already been downloaded by running the program in the full mode first. In the analysis only mode the program runs various forms of analysis on the data and stores the created artificats under subdirectories in the output folder. This mode is useful if only the analysis needs to be redone and the entire datsset does not need to be downloaded again. To run the program in analysis mode use the following command line.\n",
    "\n",
    "*python main.py -a*\n",
    "\n",
    "## Code organization and other files\n",
    "Here is a listing of everything that is included alongwith a brief description.\n",
    "\n",
    "| File name     | Notes         |\n",
    "| ------------- |-------------|---------------| \n",
    "| main.py     |  Everything starts from here, as the name suggests this is the main file of the package. This is what we run. It is organized to run a series of functions like a data science pipeline, for example get_data, clean_data and so on. It internally invokes the wb (for world bank) module and the sb (for Starbucks) module to do the same tasks for the Worldbank and Starbucks datasets respectively.|\n",
    "|README.md| This is the readme markdown file for Github.|\n",
    "|LICENCE.md| This is the license markdown file for Github. The SBS project is available under MIT license.|\n",
    "|countries.csv| A CSV file which provides the country to continent mapping.|\n",
    "|data.csv| A CSV file which provides the country code to country name mapping. Downloaded offline.|\n",
    "|WDI_Series.csv| A CSV file containing the name and description of World Development Indicators (WDI) selected for use in this project. Created offline.|\n",
    "|wb| Directory for the world bank submodule|\n",
    "|wb\\wb.py| Main file for the WorldBank submodule. It contains functions to run the data science pipeline for the WorldBank dataset.|\n",
    "|wb\\wb_check_quality.py| This file performs the data quality checks i.e. missing data and invalid data checks for the WorldBank data. It internally invokes the utils.py module for some of the data validation checks.|\n",
    "|sb\\sb.py| Main file for the Starbucks submodule. It contains functions to run the data science pipeline for the Starbucks dataset. It also contains code for feature generation.|\n",
    "|sb\\sb_check_quality.py| This file performs the data quality checks i.e. missing data and invalid data checks for the Starbucks data. It internally invokes the utils.py module for some of the data validation checks. Many of the data validation checks done are specific to the Starbucks data set and are included in this file.|\n",
    "|common| Directory for the common submodule.|\n",
    "|common\\utils.py| This file contains utility function that are common to both the modules and so this is the once place to implement these functions. Several data validation checks are also included in this file.|\n",
    "|common\\logger.py| This file creates a logger object using the Python logging module. It sets the format specifier so that the trace messages have the desired format (timestamp, module name, etc.) and also takes care of attaching two handlers, one for the console and one for the log file so all traces go to both the console and the log file.|\n",
    "|common\\globals.py| This file contains the common constants and even some variables that are common across modules. Think of this is a common header file.|\n",
    "|output| This is the folder which contains all the output (including log file) produced by the SBS program. The SBS program creates this directory if not present.|\n",
    "|output\\SBS.log| This is the log file created upon running the program. This file also contained more detailed DQS metrics i.e. missing data and invalid data related errors for individual features.|\n",
    "|output\\SB_data_as_downloaded.csv|This is the CSV version of the Starbucks data downloaded via the Socrata API.|\n",
    "|output\\SB_data_w_features.csv| This is the CSV version of Starbucks data plus with the new features added to it (last three columns) as part of the feature creation activity.|\n",
    "|output\\WB_data_as_downloaded.csv|This is the CSV version of the WorldBank data downloaded via the WB API.|\n",
    "|output\\dqs.csv| This is the CSV file which contains the data quality score for both the datasets.|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note about the structure of the output folder.\n",
    "------------------------------------------------\n",
    "The output folder contains several csv file which correspond to data as downloaded, cleaned data and combined data.\n",
    "- WDI_SB.csv: this is the combined dataset, all analysis is run on this dataset.\n",
    "- DQS: this folder contains various dqs generated at different stages, contains multiple files, the contents are self-explnatory. The Jupyter notebooks displays the        contents of these files.\n",
    "- EDA: this folder ontains artifacts from the exploratory data analysis, which include csv files and plots, the contents are self-explnatory. The Jupyter notebooks            displays the contents of these files.\n",
    "- scatter: this folder contains various scatter plots generated for the combined dataset. Specifically, the plots between number of Starbucks store and the WDI            indicators. \n",
    "- regression: this folder contain various artifacts (csv files and plots) generated as part of regression.\n",
    "- clustering: this folder contains various artifacts (csv files and plots) generated as part of clustering.\n",
    "- classification: this folder is a placeholder for artificats created as part of classification. Currently the classification results can be seen in the Jupyter                   notebook and the SBS.log file.\n",
    "- association_rules: this folder contains various artifacts (csv files only) generated as part of association rule mining. These csv files are included as part of the                      Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinining the two datasets\n",
    "The WorldBank WDI indicators dataset and the Starbucks datasets are combined together for hypothesis testing and machine learning described later in this notebook. \n",
    "To combine the two datasets we first **filter out** all the countries from the WorldBank dataset which do not have a Starbucks store i.e. we keep only those countries in the combined dataset that have at least one Starbucks store. This combined dataset is then cleaned (described later in this document) and all analysis is run on it. There are 72 countries in the world where Starbucks is present so the combined dataset contains 72 rows and 48 columns (more on the column count later). \n",
    "The following fields *computed* from the Starbucks dataset are added to the combined dataset:\n",
    "\n",
    "| Feature     | Description         |\n",
    "| ------------- |-------------|---------------| \n",
    "| Num.Starbucks.Stores    |  Total number of Starbucks stores in the country|\n",
    "| Num.Starbucks.Stores.Categorical  | Total number of Starbucks stores in the country categorized as VL (VeryLow), L (Low), Medium (M), High(H), Very High(VH)|\n",
    "|Starbucks.Store.Density\t|Number of Starbucks stores per 100,000 people|\n",
    "|Starbucks.Store.Density.Categorical|Number of Starbucks stores per 100,000 people categorized as VL (VeryLow), L (Low), Medium (M), High(H), Very High(VH)|\n",
    "|Exists.inMultiple.Cities|True if Starbucks stores exists in multiple cities in a country, False otherwise|\n",
    "|Ownership.Type.Mixed|True if multiple ownership types exists for Starbucks stores in a country (Franchisee owned, Starbucks owned etc).|\n",
    "|Continent| Name of the continent in which the country exists in|\n",
    "|MultipleBrands|True if Starbucks exists as multiple brands in the country, False otherwise|\n",
    "\n",
    "The shape of the combined dataset is (72,48). This includes 35 WDI (World Development Indicators) from the WorldBank datset.\n",
    "\n",
    "For converting continous columns to categorical a quantile (percentile) based binning strategy is used.\n",
    "\n",
    "| Range     | Category         |\n",
    "| ------------- |-------------|\n",
    "| <= 10th percentile   |  Very Low (VL)|\n",
    "| <=20th percentile   | Low (L)|\n",
    "| <=60th percentile | Medium (M)|\n",
    "| <= 80th percentile| High (H)|\n",
    "|beyond 80th percentile| Very High (VH)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "This section describes various types of exploratory analysis run on the two datasets as well as on the combined dataset before getting into the hypothesis building and machine learning part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistical Analysis and data cleaning\n",
    "This section provides basic statistical analysis for both the WorldBank dataset and the StarBucks datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean, mode, median, standard deviation\n",
    "The WorldBank dataset consists of all numeric values except for the country code, country name and the derived feature for ease of doing business (categorical). The country code and name are excluded from the statistical analysis since there is onnly one row per country in the dataset. The statistical measures for each feature are listed below. Some of the features show NaN for the statistical measures, that is because there is no data at all for these features (they are subsequently removed from the dataset, described later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>mode</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC.TAX.METG</td>\n",
       "      <td>1.467218e+00</td>\n",
       "      <td>1.435000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>5.794943e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SL.EMP.TOTL.SP.FE.NE.ZS</td>\n",
       "      <td>4.530958e+01</td>\n",
       "      <td>4.750000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.220095e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT.NET.USER.P2</td>\n",
       "      <td>4.755657e+01</td>\n",
       "      <td>4.688564e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>2.769172e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>2.970610e+08</td>\n",
       "      <td>1.009667e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>9.376648e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VC.PKP.TOTL.UN</td>\n",
       "      <td>4.955647e+03</td>\n",
       "      <td>7.930000e+02</td>\n",
       "      <td>0</td>\n",
       "      <td>6.086792e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IC.GOV.DURS.ZS</td>\n",
       "      <td>8.830681e+00</td>\n",
       "      <td>8.956250e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>4.266066e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IC.REG.PROC</td>\n",
       "      <td>6.938789e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.928893e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IC.LGL.CRED.XQ</td>\n",
       "      <td>5.110327e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.741384e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EG.ELC.ACCS.UR.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IC.BUS.NDNS.ZS</td>\n",
       "      <td>3.674968e+00</td>\n",
       "      <td>2.290769e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>4.270679e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EN.URB.LCTY.UR.ZS</td>\n",
       "      <td>2.988715e+01</td>\n",
       "      <td>2.684948e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.682697e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SL.EMP.TOTL.SP.MA.NE.ZS</td>\n",
       "      <td>6.361581e+01</td>\n",
       "      <td>6.384569e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>8.498745e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IC.ELC.DURS</td>\n",
       "      <td>3.351995e+01</td>\n",
       "      <td>3.074444e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>2.697875e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>5.862963e+01</td>\n",
       "      <td>5.837484e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>2.313314e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NY.GNP.PCAP.CD</td>\n",
       "      <td>1.267481e+04</td>\n",
       "      <td>5.585000e+03</td>\n",
       "      <td>0</td>\n",
       "      <td>1.769632e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SL.TLF.ACTI.1524.NE.ZS</td>\n",
       "      <td>4.255292e+01</td>\n",
       "      <td>4.113231e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.256837e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>IC.WRH.DURS</td>\n",
       "      <td>1.611548e+02</td>\n",
       "      <td>1.540000e+02</td>\n",
       "      <td>0</td>\n",
       "      <td>8.145088e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IC.TAX.TOTL.CP.ZS</td>\n",
       "      <td>4.088493e+01</td>\n",
       "      <td>3.927222e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.914462e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>4.012033e+01</td>\n",
       "      <td>4.172000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>9.002209e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SI.POV.2DAY</td>\n",
       "      <td>1.790452e+01</td>\n",
       "      <td>1.129000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>2.032023e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IC.ELC.TIME</td>\n",
       "      <td>9.773877e+01</td>\n",
       "      <td>8.700000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>6.537607e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>IC.TAX.OTHR.CP.ZS</td>\n",
       "      <td>8.843270e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.803673e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IC.ISV.DURS</td>\n",
       "      <td>2.597505e+00</td>\n",
       "      <td>2.581818e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.055514e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FB.ATM.TOTL.P5</td>\n",
       "      <td>4.842994e+01</td>\n",
       "      <td>4.170072e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>4.303515e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SL.UEM.NEET.ZS</td>\n",
       "      <td>1.347953e+01</td>\n",
       "      <td>1.228000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>6.136543e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>EG.ELC.RNEW.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IQ.WEF.PORT.XQ</td>\n",
       "      <td>3.958079e+00</td>\n",
       "      <td>3.904944e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.123580e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IC.TAX.GIFT.ZS</td>\n",
       "      <td>1.529794e+01</td>\n",
       "      <td>1.701000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>8.678293e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IQ.WEF.CUST.XQ</td>\n",
       "      <td>3.983466e+00</td>\n",
       "      <td>3.813284e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>8.156708e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EN.ATM.PM25.MC.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>BX.KLT.DINV.WD.GD.ZS</td>\n",
       "      <td>4.660374e+00</td>\n",
       "      <td>2.556654e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>8.348388e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>TX.VAL.TECH.CD</td>\n",
       "      <td>7.403359e+10</td>\n",
       "      <td>2.120983e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2.555456e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>EN.CLC.MDAT.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>SL.EMP.WORK.ZS</td>\n",
       "      <td>8.124434e+01</td>\n",
       "      <td>8.417780e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>9.151259e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SH.STA.ACSN.UR</td>\n",
       "      <td>7.898257e+01</td>\n",
       "      <td>8.797773e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>2.325356e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>WP_time_01.1</td>\n",
       "      <td>5.330584e+01</td>\n",
       "      <td>5.126150e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>3.125552e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>SL.IND.EMPL.ZS</td>\n",
       "      <td>2.338136e+01</td>\n",
       "      <td>2.320000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>6.635862e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>EN.POP.SLUM.UR.ZS</td>\n",
       "      <td>4.404918e+01</td>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>2.234641e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>SP.POP.1564.TO.ZS</td>\n",
       "      <td>6.374940e+01</td>\n",
       "      <td>6.536425e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>6.471699e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>SE.ADT.LITR.ZS</td>\n",
       "      <td>8.416093e+01</td>\n",
       "      <td>9.397165e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.876130e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>FI.RES.TOTL.DT.ZS</td>\n",
       "      <td>9.210792e+01</td>\n",
       "      <td>4.777428e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>2.829776e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>IC.BUS.DISC.XQ</td>\n",
       "      <td>5.470902e+00</td>\n",
       "      <td>5.487101e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.288636e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>CM.MKT.TRAD.GD.ZS</td>\n",
       "      <td>6.147025e+01</td>\n",
       "      <td>1.907471e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.006608e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>SL.SRV.EMPL.ZS</td>\n",
       "      <td>6.585253e+01</td>\n",
       "      <td>6.905000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.252427e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>NE.CON.PRVT.KD.ZG</td>\n",
       "      <td>2.794388e+00</td>\n",
       "      <td>3.038269e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>4.618765e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>AG.LND.TOTL.UR.K2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>FP.CPI.TOTL.ZG</td>\n",
       "      <td>3.764240e+00</td>\n",
       "      <td>1.835994e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>9.904829e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>ST.INT.RCPT.XP.ZS</td>\n",
       "      <td>1.426136e+01</td>\n",
       "      <td>6.996199e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.748747e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>IC.REG.COST.PC.ZS</td>\n",
       "      <td>2.671969e+01</td>\n",
       "      <td>1.397500e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>3.990623e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>LP.LPI.OVRL.XQ</td>\n",
       "      <td>2.867913e+00</td>\n",
       "      <td>2.739469e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>5.229044e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>IC.FRM.BRIB.ZS</td>\n",
       "      <td>2.187780e+01</td>\n",
       "      <td>2.390000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.124342e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>IC.FRM.DURS</td>\n",
       "      <td>2.277239e+01</td>\n",
       "      <td>1.909048e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.139392e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>IT.CEL.SETS.P2</td>\n",
       "      <td>1.063206e+02</td>\n",
       "      <td>1.078952e+02</td>\n",
       "      <td>0</td>\n",
       "      <td>3.922009e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>IC.IMP.COST.CD</td>\n",
       "      <td>1.883703e+03</td>\n",
       "      <td>1.480000e+03</td>\n",
       "      <td>0</td>\n",
       "      <td>1.511363e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>1.564382e+03</td>\n",
       "      <td>1.270810e+03</td>\n",
       "      <td>0</td>\n",
       "      <td>1.127318e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>IC.TAX.LABR.CP.ZS</td>\n",
       "      <td>1.572635e+01</td>\n",
       "      <td>1.405870e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>9.984998e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>IC.ELC.OUTG</td>\n",
       "      <td>7.778588e+00</td>\n",
       "      <td>5.650000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>8.081534e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>EN.ATM.CO2E.EG.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>DT.DOD.PVLX.GN.ZS</td>\n",
       "      <td>2.090316e+01</td>\n",
       "      <td>1.743921e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.352618e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Ease.Of.Doing.Business</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>VeryLow:70</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feature          mean        median        mode  \\\n",
       "0               IC.TAX.METG  1.467218e+00  1.435000e+00           0   \n",
       "1   SL.EMP.TOTL.SP.FE.NE.ZS  4.530958e+01  4.750000e+01           0   \n",
       "2            IT.NET.USER.P2  4.755657e+01  4.688564e+01           0   \n",
       "3               SP.POP.TOTL  2.970610e+08  1.009667e+07           0   \n",
       "4            VC.PKP.TOTL.UN  4.955647e+03  7.930000e+02           0   \n",
       "5            IC.GOV.DURS.ZS  8.830681e+00  8.956250e+00           0   \n",
       "6               IC.REG.PROC  6.938789e+00  7.000000e+00           0   \n",
       "7            IC.LGL.CRED.XQ  5.110327e+00  5.000000e+00           0   \n",
       "8         EG.ELC.ACCS.UR.ZS           NaN           NaN           0   \n",
       "9            IC.BUS.NDNS.ZS  3.674968e+00  2.290769e+00           0   \n",
       "10        EN.URB.LCTY.UR.ZS  2.988715e+01  2.684948e+01           0   \n",
       "11  SL.EMP.TOTL.SP.MA.NE.ZS  6.361581e+01  6.384569e+01           0   \n",
       "12              IC.ELC.DURS  3.351995e+01  3.074444e+01           0   \n",
       "13        SP.URB.TOTL.IN.ZS  5.862963e+01  5.837484e+01           0   \n",
       "14           NY.GNP.PCAP.CD  1.267481e+04  5.585000e+03           0   \n",
       "15   SL.TLF.ACTI.1524.NE.ZS  4.255292e+01  4.113231e+01           0   \n",
       "16              IC.WRH.DURS  1.611548e+02  1.540000e+02           0   \n",
       "17        IC.TAX.TOTL.CP.ZS  4.088493e+01  3.927222e+01           0   \n",
       "18              SI.POV.GINI  4.012033e+01  4.172000e+01           0   \n",
       "19              SI.POV.2DAY  1.790452e+01  1.129000e+01           0   \n",
       "20              IC.ELC.TIME  9.773877e+01  8.700000e+01           0   \n",
       "21        IC.TAX.OTHR.CP.ZS  8.843270e+00  3.000000e+00           0   \n",
       "22              IC.ISV.DURS  2.597505e+00  2.581818e+00           0   \n",
       "23           FB.ATM.TOTL.P5  4.842994e+01  4.170072e+01           0   \n",
       "24           SL.UEM.NEET.ZS  1.347953e+01  1.228000e+01           0   \n",
       "25           EG.ELC.RNEW.ZS           NaN           NaN           0   \n",
       "26           IQ.WEF.PORT.XQ  3.958079e+00  3.904944e+00           0   \n",
       "27           IC.TAX.GIFT.ZS  1.529794e+01  1.701000e+01           0   \n",
       "28           IQ.WEF.CUST.XQ  3.983466e+00  3.813284e+00           0   \n",
       "29        EN.ATM.PM25.MC.ZS           NaN           NaN           0   \n",
       "..                      ...           ...           ...         ...   \n",
       "53     BX.KLT.DINV.WD.GD.ZS  4.660374e+00  2.556654e+00           0   \n",
       "54           TX.VAL.TECH.CD  7.403359e+10  2.120983e+08           0   \n",
       "55           EN.CLC.MDAT.ZS           NaN           NaN           0   \n",
       "56           SL.EMP.WORK.ZS  8.124434e+01  8.417780e+01           0   \n",
       "57           SH.STA.ACSN.UR  7.898257e+01  8.797773e+01           0   \n",
       "58             WP_time_01.1  5.330584e+01  5.126150e+01           0   \n",
       "59           SL.IND.EMPL.ZS  2.338136e+01  2.320000e+01           0   \n",
       "60        EN.POP.SLUM.UR.ZS  4.404918e+01  4.300000e+01           0   \n",
       "61        SP.POP.1564.TO.ZS  6.374940e+01  6.536425e+01           0   \n",
       "62           SE.ADT.LITR.ZS  8.416093e+01  9.397165e+01           0   \n",
       "63        FI.RES.TOTL.DT.ZS  9.210792e+01  4.777428e+01           0   \n",
       "64           IC.BUS.DISC.XQ  5.470902e+00  5.487101e+00           0   \n",
       "65        CM.MKT.TRAD.GD.ZS  6.147025e+01  1.907471e+01           0   \n",
       "66           SL.SRV.EMPL.ZS  6.585253e+01  6.905000e+01           0   \n",
       "67        NE.CON.PRVT.KD.ZG  2.794388e+00  3.038269e+00           0   \n",
       "68        AG.LND.TOTL.UR.K2           NaN           NaN           0   \n",
       "69           FP.CPI.TOTL.ZG  3.764240e+00  1.835994e+00           0   \n",
       "70        ST.INT.RCPT.XP.ZS  1.426136e+01  6.996199e+00           0   \n",
       "71        IC.REG.COST.PC.ZS  2.671969e+01  1.397500e+01           0   \n",
       "72           LP.LPI.OVRL.XQ  2.867913e+00  2.739469e+00           0   \n",
       "73           IC.FRM.BRIB.ZS  2.187780e+01  2.390000e+01           0   \n",
       "74              IC.FRM.DURS  2.277239e+01  1.909048e+01           0   \n",
       "75           IT.CEL.SETS.P2  1.063206e+02  1.078952e+02           0   \n",
       "76           IC.IMP.COST.CD  1.883703e+03  1.480000e+03           0   \n",
       "77           IC.EXP.COST.CD  1.564382e+03  1.270810e+03           0   \n",
       "78        IC.TAX.LABR.CP.ZS  1.572635e+01  1.405870e+01           0   \n",
       "79              IC.ELC.OUTG  7.778588e+00  5.650000e+00           0   \n",
       "80        EN.ATM.CO2E.EG.ZS           NaN           NaN           0   \n",
       "81        DT.DOD.PVLX.GN.ZS  2.090316e+01  1.743921e+01           0   \n",
       "82   Ease.Of.Doing.Business  0.000000e+00  0.000000e+00  VeryLow:70   \n",
       "\n",
       "          stddev  \n",
       "0   5.794943e-01  \n",
       "1   1.220095e+01  \n",
       "2   2.769172e+01  \n",
       "3   9.376648e+08  \n",
       "4   6.086792e+03  \n",
       "5   4.266066e+00  \n",
       "6   2.928893e+00  \n",
       "7   2.741384e+00  \n",
       "8            NaN  \n",
       "9   4.270679e+00  \n",
       "10  1.682697e+01  \n",
       "11  8.498745e+00  \n",
       "12  2.697875e+01  \n",
       "13  2.313314e+01  \n",
       "14  1.769632e+04  \n",
       "15  1.256837e+01  \n",
       "16  8.145088e+01  \n",
       "17  1.914462e+01  \n",
       "18  9.002209e+00  \n",
       "19  2.032023e+01  \n",
       "20  6.537607e+01  \n",
       "21  1.803673e+01  \n",
       "22  1.055514e+00  \n",
       "23  4.303515e+01  \n",
       "24  6.136543e+00  \n",
       "25           NaN  \n",
       "26  1.123580e+00  \n",
       "27  8.678293e+00  \n",
       "28  8.156708e-01  \n",
       "29           NaN  \n",
       "..           ...  \n",
       "53  8.348388e+00  \n",
       "54  2.555456e+11  \n",
       "55           NaN  \n",
       "56  9.151259e+00  \n",
       "57  2.325356e+01  \n",
       "58  3.125552e+01  \n",
       "59  6.635862e+00  \n",
       "60  2.234641e+01  \n",
       "61  6.471699e+00  \n",
       "62  1.876130e+01  \n",
       "63  2.829776e+02  \n",
       "64  2.288636e+00  \n",
       "65  1.006608e+02  \n",
       "66  1.252427e+01  \n",
       "67  4.618765e+00  \n",
       "68           NaN  \n",
       "69  9.904829e+00  \n",
       "70  1.748747e+01  \n",
       "71  3.990623e+01  \n",
       "72  5.229044e-01  \n",
       "73  1.124342e+01  \n",
       "74  1.139392e+01  \n",
       "75  3.922009e+01  \n",
       "76  1.511363e+03  \n",
       "77  1.127318e+03  \n",
       "78  9.984998e+00  \n",
       "79  8.081534e+00  \n",
       "80           NaN  \n",
       "81  1.352618e+01  \n",
       "82  0.000000e+00  \n",
       "\n",
       "[83 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "WB_EDA_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/EDA/WB_EDA.csv'\n",
    "df = pd.read_csv(WB_EDA_URL)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Starbucks dataset consists of all categorical features, therefore only a mode calculation is valid. Several features such as street names are excluded from the mode calculation as well. There are some intresting observations to be made \n",
    "1. The maximum number of Starbucks store in a single city are in a city outside of the U.S.\n",
    "2. The maximum number of Starbucks store in the U.S. and in the Eastern Timezone.\n",
    "3. The maximum number of Starbucks stores (about 70%) occur in countries with very high ease of doing business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>mode</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brand</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Starbucks:24733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>city</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>上海市:498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>country</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US:13486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ownership_type</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CO:11777</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>timezone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Eastern Standard Time:5460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>continent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>North America:15568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>on_airport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False:24641</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eodb_category</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VH:15549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  mean  median                        mode  stddev\n",
       "0           brand     0       0             Starbucks:24733       0\n",
       "1            city     0       0                     上海市:498       0\n",
       "2         country     0       0                    US:13486       0\n",
       "3  ownership_type     0       0                    CO:11777       0\n",
       "4        timezone     0       0  Eastern Standard Time:5460       0\n",
       "5       continent     0       0         North America:15568       0\n",
       "6      on_airport     0       0                 False:24641       0\n",
       "7   eodb_category     0       0                    VH:15549       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "SB_EDA_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/EDA/SB_EDA.csv'\n",
    "df = pd.read_csv(SB_EDA_URL)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "Outlier detection is performed on the WorldBank dataset. We determine outliers using a simple rule, anything outside of the 3rd standard deviation is an outlier. The results of outlier detection are provided below. Note that while there are values which get categorized as outliers but they are still valid values (as understood by human inspection of the data) therefore no outlier handling as such (smoothning, removal etc) is done for these values. Also given that these are valid values they cannot be ignored as they may have a significant role in the machine learning algorithms.\n",
    "\n",
    "No outlier detection is performed on the Starbucks dataset as it contains only categorical values (values outside of well defined valid values set are considered invalid and that analysis has already been performed as part of the data quality detection phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>entry</th>\n",
       "      <th>field</th>\n",
       "      <th>value</th>\n",
       "      <th>3rdStdDev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WB</td>\n",
       "      <td>Early-demographic dividend</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>3.122703e+09</td>\n",
       "      <td>2.812994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WB</td>\n",
       "      <td>Middle income</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>5.521157e+09</td>\n",
       "      <td>2.812994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WB</td>\n",
       "      <td>IBRD only</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>4.542581e+09</td>\n",
       "      <td>2.812994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WB</td>\n",
       "      <td>Low &amp; middle income</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>6.159443e+09</td>\n",
       "      <td>2.812994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WB</td>\n",
       "      <td>IDA &amp; IBRD total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>6.183635e+09</td>\n",
       "      <td>2.812994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WB</td>\n",
       "      <td>World</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>7.346633e+09</td>\n",
       "      <td>2.812994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WB</td>\n",
       "      <td>Bhutan</td>\n",
       "      <td>IC.GOV.DURS.ZS</td>\n",
       "      <td>2.880000e+01</td>\n",
       "      <td>1.279820e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WB</td>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>IC.REG.PROC</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>8.786678e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WB</td>\n",
       "      <td>Venezuela, RB</td>\n",
       "      <td>IC.REG.PROC</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>8.786678e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WB</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>IC.REG.PROC</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>8.786678e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WB</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>IC.BUS.NDNS.ZS</td>\n",
       "      <td>1.663000e+01</td>\n",
       "      <td>1.281204e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WB</td>\n",
       "      <td>Malta</td>\n",
       "      <td>IC.BUS.NDNS.ZS</td>\n",
       "      <td>1.726000e+01</td>\n",
       "      <td>1.281204e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WB</td>\n",
       "      <td>Hong Kong SAR, China</td>\n",
       "      <td>IC.BUS.NDNS.ZS</td>\n",
       "      <td>3.130000e+01</td>\n",
       "      <td>1.281204e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WB</td>\n",
       "      <td>Hong Kong SAR, China</td>\n",
       "      <td>EN.URB.LCTY.UR.ZS</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>5.048091e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WB</td>\n",
       "      <td>Macao SAR, China</td>\n",
       "      <td>EN.URB.LCTY.UR.ZS</td>\n",
       "      <td>9.945780e+01</td>\n",
       "      <td>5.048091e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WB</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>EN.URB.LCTY.UR.ZS</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>5.048091e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WB</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>IC.ELC.DURS</td>\n",
       "      <td>1.943000e+02</td>\n",
       "      <td>8.093626e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WB</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>NY.GNP.PCAP.CD</td>\n",
       "      <td>7.700000e+04</td>\n",
       "      <td>5.308897e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WB</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>NY.GNP.PCAP.CD</td>\n",
       "      <td>8.418000e+04</td>\n",
       "      <td>5.308897e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WB</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>NY.GNP.PCAP.CD</td>\n",
       "      <td>8.543000e+04</td>\n",
       "      <td>5.308897e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WB</td>\n",
       "      <td>Norway</td>\n",
       "      <td>NY.GNP.PCAP.CD</td>\n",
       "      <td>9.382000e+04</td>\n",
       "      <td>5.308897e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>WB</td>\n",
       "      <td>Macao SAR, China</td>\n",
       "      <td>NY.GNP.PCAP.CD</td>\n",
       "      <td>7.630000e+04</td>\n",
       "      <td>5.308897e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>WB</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>IC.WRH.DURS</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>2.443526e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>WB</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>IC.WRH.DURS</td>\n",
       "      <td>6.170000e+02</td>\n",
       "      <td>2.443526e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>WB</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>IC.WRH.DURS</td>\n",
       "      <td>6.520000e+02</td>\n",
       "      <td>2.443526e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>WB</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>IC.WRH.DURS</td>\n",
       "      <td>4.257000e+02</td>\n",
       "      <td>2.443526e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>WB</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>IC.WRH.DURS</td>\n",
       "      <td>4.480000e+02</td>\n",
       "      <td>2.443526e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>WB</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>IC.TAX.TOTL.CP.ZS</td>\n",
       "      <td>1.374000e+02</td>\n",
       "      <td>5.743387e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>WB</td>\n",
       "      <td>Comoros</td>\n",
       "      <td>IC.TAX.TOTL.CP.ZS</td>\n",
       "      <td>2.165000e+02</td>\n",
       "      <td>5.743387e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>WB</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>IC.ELC.TIME</td>\n",
       "      <td>4.289000e+02</td>\n",
       "      <td>1.961282e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>WB</td>\n",
       "      <td>Macao SAR, China</td>\n",
       "      <td>ST.INT.RCPT.XP.ZS</td>\n",
       "      <td>9.367504e+01</td>\n",
       "      <td>5.246241e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>WB</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>IC.REG.COST.PC.ZS</td>\n",
       "      <td>3.301000e+02</td>\n",
       "      <td>1.197187e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>WB</td>\n",
       "      <td>Central African Republic</td>\n",
       "      <td>IC.REG.COST.PC.ZS</td>\n",
       "      <td>2.040000e+02</td>\n",
       "      <td>1.197187e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>WB</td>\n",
       "      <td>Haiti</td>\n",
       "      <td>IC.REG.COST.PC.ZS</td>\n",
       "      <td>2.353000e+02</td>\n",
       "      <td>1.197187e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>WB</td>\n",
       "      <td>Chad</td>\n",
       "      <td>IC.REG.COST.PC.ZS</td>\n",
       "      <td>1.504000e+02</td>\n",
       "      <td>1.197187e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>WB</td>\n",
       "      <td>Djibouti</td>\n",
       "      <td>IC.REG.COST.PC.ZS</td>\n",
       "      <td>1.681000e+02</td>\n",
       "      <td>1.197187e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>WB</td>\n",
       "      <td>Mauritania</td>\n",
       "      <td>IC.FRM.DURS</td>\n",
       "      <td>6.460000e+01</td>\n",
       "      <td>3.418175e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>WB</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>IT.CEL.SETS.P2</td>\n",
       "      <td>2.317632e+02</td>\n",
       "      <td>1.176603e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>WB</td>\n",
       "      <td>Hong Kong SAR, China</td>\n",
       "      <td>IT.CEL.SETS.P2</td>\n",
       "      <td>2.288316e+02</td>\n",
       "      <td>1.176603e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>WB</td>\n",
       "      <td>Macao SAR, China</td>\n",
       "      <td>IT.CEL.SETS.P2</td>\n",
       "      <td>3.244408e+02</td>\n",
       "      <td>1.176603e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>WB</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>IC.IMP.COST.CD</td>\n",
       "      <td>7.060000e+03</td>\n",
       "      <td>4.534090e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>WB</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>IC.IMP.COST.CD</td>\n",
       "      <td>9.285000e+03</td>\n",
       "      <td>4.534090e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>WB</td>\n",
       "      <td>Congo, Rep.</td>\n",
       "      <td>IC.IMP.COST.CD</td>\n",
       "      <td>7.590000e+03</td>\n",
       "      <td>4.534090e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>WB</td>\n",
       "      <td>Tajikistan</td>\n",
       "      <td>IC.IMP.COST.CD</td>\n",
       "      <td>1.065000e+04</td>\n",
       "      <td>4.534090e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>WB</td>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>IC.IMP.COST.CD</td>\n",
       "      <td>6.452000e+03</td>\n",
       "      <td>4.534090e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>WB</td>\n",
       "      <td>Chad</td>\n",
       "      <td>IC.IMP.COST.CD</td>\n",
       "      <td>9.025000e+03</td>\n",
       "      <td>4.534090e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>WB</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>5.165000e+03</td>\n",
       "      <td>3.381954e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>WB</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>5.335000e+03</td>\n",
       "      <td>3.381954e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>WB</td>\n",
       "      <td>Tajikistan</td>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>9.050000e+03</td>\n",
       "      <td>3.381954e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>WB</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>5.045000e+03</td>\n",
       "      <td>3.381954e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>WB</td>\n",
       "      <td>Central African Republic</td>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>5.490000e+03</td>\n",
       "      <td>3.381954e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>WB</td>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>5.090000e+03</td>\n",
       "      <td>3.381954e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>WB</td>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>5.285000e+03</td>\n",
       "      <td>3.381954e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>WB</td>\n",
       "      <td>Chad</td>\n",
       "      <td>IC.EXP.COST.CD</td>\n",
       "      <td>6.615000e+03</td>\n",
       "      <td>3.381954e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>WB</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>IC.TAX.LABR.CP.ZS</td>\n",
       "      <td>4.940000e+01</td>\n",
       "      <td>2.995499e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>WB</td>\n",
       "      <td>France</td>\n",
       "      <td>IC.TAX.LABR.CP.ZS</td>\n",
       "      <td>5.350000e+01</td>\n",
       "      <td>2.995499e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>WB</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>IC.ELC.OUTG</td>\n",
       "      <td>3.280000e+01</td>\n",
       "      <td>2.424460e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>WB</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>IC.ELC.OUTG</td>\n",
       "      <td>4.190000e+01</td>\n",
       "      <td>2.424460e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>WB</td>\n",
       "      <td>Bhutan</td>\n",
       "      <td>DT.DOD.PVLX.GN.ZS</td>\n",
       "      <td>6.815542e+01</td>\n",
       "      <td>4.057852e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>WB</td>\n",
       "      <td>Cabo Verde</td>\n",
       "      <td>DT.DOD.PVLX.GN.ZS</td>\n",
       "      <td>6.770737e+01</td>\n",
       "      <td>4.057852e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset                       entry              field         value  \\\n",
       "0        WB  Early-demographic dividend        SP.POP.TOTL  3.122703e+09   \n",
       "1        WB               Middle income        SP.POP.TOTL  5.521157e+09   \n",
       "2        WB                   IBRD only        SP.POP.TOTL  4.542581e+09   \n",
       "3        WB         Low & middle income        SP.POP.TOTL  6.159443e+09   \n",
       "4        WB            IDA & IBRD total        SP.POP.TOTL  6.183635e+09   \n",
       "5        WB                       World        SP.POP.TOTL  7.346633e+09   \n",
       "6        WB                      Bhutan     IC.GOV.DURS.ZS  2.880000e+01   \n",
       "7        WB           Equatorial Guinea        IC.REG.PROC  1.800000e+01   \n",
       "8        WB               Venezuela, RB        IC.REG.PROC  1.700000e+01   \n",
       "9        WB                 Philippines        IC.REG.PROC  1.600000e+01   \n",
       "10       WB                 New Zealand     IC.BUS.NDNS.ZS  1.663000e+01   \n",
       "11       WB                       Malta     IC.BUS.NDNS.ZS  1.726000e+01   \n",
       "12       WB        Hong Kong SAR, China     IC.BUS.NDNS.ZS  3.130000e+01   \n",
       "13       WB        Hong Kong SAR, China  EN.URB.LCTY.UR.ZS  1.000000e+02   \n",
       "14       WB            Macao SAR, China  EN.URB.LCTY.UR.ZS  9.945780e+01   \n",
       "15       WB                   Singapore  EN.URB.LCTY.UR.ZS  1.000000e+02   \n",
       "16       WB                    Ethiopia        IC.ELC.DURS  1.943000e+02   \n",
       "17       WB                  Luxembourg     NY.GNP.PCAP.CD  7.700000e+04   \n",
       "18       WB                 Switzerland     NY.GNP.PCAP.CD  8.418000e+04   \n",
       "19       WB                       Qatar     NY.GNP.PCAP.CD  8.543000e+04   \n",
       "20       WB                      Norway     NY.GNP.PCAP.CD  9.382000e+04   \n",
       "21       WB            Macao SAR, China     NY.GNP.PCAP.CD  7.630000e+04   \n",
       "22       WB                    Barbados        IC.WRH.DURS  4.420000e+02   \n",
       "23       WB                      Cyprus        IC.WRH.DURS  6.170000e+02   \n",
       "24       WB                    Cambodia        IC.WRH.DURS  6.520000e+02   \n",
       "25       WB                      Brazil        IC.WRH.DURS  4.257000e+02   \n",
       "26       WB                    Zimbabwe        IC.WRH.DURS  4.480000e+02   \n",
       "27       WB                   Argentina  IC.TAX.TOTL.CP.ZS  1.374000e+02   \n",
       "28       WB                     Comoros  IC.TAX.TOTL.CP.ZS  2.165000e+02   \n",
       "29       WB                  Bangladesh        IC.ELC.TIME  4.289000e+02   \n",
       "..      ...                         ...                ...           ...   \n",
       "118      WB            Macao SAR, China  ST.INT.RCPT.XP.ZS  9.367504e+01   \n",
       "119      WB                 South Sudan  IC.REG.COST.PC.ZS  3.301000e+02   \n",
       "120      WB    Central African Republic  IC.REG.COST.PC.ZS  2.040000e+02   \n",
       "121      WB                       Haiti  IC.REG.COST.PC.ZS  2.353000e+02   \n",
       "122      WB                        Chad  IC.REG.COST.PC.ZS  1.504000e+02   \n",
       "123      WB                    Djibouti  IC.REG.COST.PC.ZS  1.681000e+02   \n",
       "124      WB                  Mauritania        IC.FRM.DURS  6.460000e+01   \n",
       "125      WB                      Kuwait     IT.CEL.SETS.P2  2.317632e+02   \n",
       "126      WB        Hong Kong SAR, China     IT.CEL.SETS.P2  2.288316e+02   \n",
       "127      WB            Macao SAR, China     IT.CEL.SETS.P2  3.244408e+02   \n",
       "128      WB                      Zambia     IC.IMP.COST.CD  7.060000e+03   \n",
       "129      WB                 South Sudan     IC.IMP.COST.CD  9.285000e+03   \n",
       "130      WB                 Congo, Rep.     IC.IMP.COST.CD  7.590000e+03   \n",
       "131      WB                  Tajikistan     IC.IMP.COST.CD  1.065000e+04   \n",
       "132      WB                  Uzbekistan     IC.IMP.COST.CD  6.452000e+03   \n",
       "133      WB                        Chad     IC.IMP.COST.CD  9.025000e+03   \n",
       "134      WB                      Zambia     IC.EXP.COST.CD  5.165000e+03   \n",
       "135      WB                 South Sudan     IC.EXP.COST.CD  5.335000e+03   \n",
       "136      WB                  Tajikistan     IC.EXP.COST.CD  9.050000e+03   \n",
       "137      WB                 Afghanistan     IC.EXP.COST.CD  5.045000e+03   \n",
       "138      WB    Central African Republic     IC.EXP.COST.CD  5.490000e+03   \n",
       "139      WB                  Uzbekistan     IC.EXP.COST.CD  5.090000e+03   \n",
       "140      WB                  Kazakhstan     IC.EXP.COST.CD  5.285000e+03   \n",
       "141      WB                        Chad     IC.EXP.COST.CD  6.615000e+03   \n",
       "142      WB                     Belgium  IC.TAX.LABR.CP.ZS  4.940000e+01   \n",
       "143      WB                      France  IC.TAX.LABR.CP.ZS  5.350000e+01   \n",
       "144      WB                     Nigeria        IC.ELC.OUTG  3.280000e+01   \n",
       "145      WB            Papua New Guinea        IC.ELC.OUTG  4.190000e+01   \n",
       "146      WB                      Bhutan  DT.DOD.PVLX.GN.ZS  6.815542e+01   \n",
       "147      WB                  Cabo Verde  DT.DOD.PVLX.GN.ZS  6.770737e+01   \n",
       "\n",
       "        3rdStdDev  \n",
       "0    2.812994e+09  \n",
       "1    2.812994e+09  \n",
       "2    2.812994e+09  \n",
       "3    2.812994e+09  \n",
       "4    2.812994e+09  \n",
       "5    2.812994e+09  \n",
       "6    1.279820e+01  \n",
       "7    8.786678e+00  \n",
       "8    8.786678e+00  \n",
       "9    8.786678e+00  \n",
       "10   1.281204e+01  \n",
       "11   1.281204e+01  \n",
       "12   1.281204e+01  \n",
       "13   5.048091e+01  \n",
       "14   5.048091e+01  \n",
       "15   5.048091e+01  \n",
       "16   8.093626e+01  \n",
       "17   5.308897e+04  \n",
       "18   5.308897e+04  \n",
       "19   5.308897e+04  \n",
       "20   5.308897e+04  \n",
       "21   5.308897e+04  \n",
       "22   2.443526e+02  \n",
       "23   2.443526e+02  \n",
       "24   2.443526e+02  \n",
       "25   2.443526e+02  \n",
       "26   2.443526e+02  \n",
       "27   5.743387e+01  \n",
       "28   5.743387e+01  \n",
       "29   1.961282e+02  \n",
       "..            ...  \n",
       "118  5.246241e+01  \n",
       "119  1.197187e+02  \n",
       "120  1.197187e+02  \n",
       "121  1.197187e+02  \n",
       "122  1.197187e+02  \n",
       "123  1.197187e+02  \n",
       "124  3.418175e+01  \n",
       "125  1.176603e+02  \n",
       "126  1.176603e+02  \n",
       "127  1.176603e+02  \n",
       "128  4.534090e+03  \n",
       "129  4.534090e+03  \n",
       "130  4.534090e+03  \n",
       "131  4.534090e+03  \n",
       "132  4.534090e+03  \n",
       "133  4.534090e+03  \n",
       "134  3.381954e+03  \n",
       "135  3.381954e+03  \n",
       "136  3.381954e+03  \n",
       "137  3.381954e+03  \n",
       "138  3.381954e+03  \n",
       "139  3.381954e+03  \n",
       "140  3.381954e+03  \n",
       "141  3.381954e+03  \n",
       "142  2.995499e+01  \n",
       "143  2.995499e+01  \n",
       "144  2.424460e+01  \n",
       "145  2.424460e+01  \n",
       "146  4.057852e+01  \n",
       "147  4.057852e+01  \n",
       "\n",
       "[148 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "WB_OUTLIER_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/EDA/WB_outliers.csv'\n",
    "df = pd.read_csv(WB_OUTLIER_URL)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin the data\n",
    "As part of the analysis that follows several continous/discrete features were binned (as described below this is needed for association rule mining, feature selection etc.). Here we show one specific feature IC.BUS.EASE.XQ which quantifies the ease of doing business in a country (1 being highest, larger the value lower the ease of doing business) into a categorical value. Numeric values get mapped to categories as follows:\n",
    "\n",
    "| Range     | Category         |\n",
    "| ------------- |-------------|---------------| \n",
    "| [1,11)    |  VeryHigh|\n",
    "| [11,31)   | High|\n",
    "| [31,91) | Medium |\n",
    "| [91, 131)| Low|\n",
    "|[131, beyond| VeryLow|\n",
    "\n",
    "The benefits of binning this feature is for exploratory data analysis to answer questions like what % of Starbucks store are in countries with high or very high ease of business. The method used provides a simple strategy to bin the data, although it would require a subject matter expert to say what the most appropriate bin edges are.\n",
    "\n",
    "Here is an excerpt from the WorldBank data showing the new column created as a result of binning along with the original feature (first 5 rows shown). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>IC.BUS.EASE.XQ</th>\n",
       "      <th>Ease.Of.Doing.Business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>VeryLow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Early-demographic dividend</td>\n",
       "      <td>117.868852</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small states</td>\n",
       "      <td>107.354167</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>VeryLow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  IC.BUS.EASE.XQ Ease.Of.Doing.Business\n",
       "0                     Belgium       43.000000                 Medium\n",
       "1           Equatorial Guinea      180.000000                VeryLow\n",
       "2  Early-demographic dividend      117.868852                    Low\n",
       "3                Small states      107.354167                    Low\n",
       "4                  Bangladesh      174.000000                VeryLow"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "WB_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/WB_data_w_features.csv'\n",
    "df = pd.read_csv(WB_URL)\n",
    "df[['name', 'IC.BUS.EASE.XQ', 'Ease.Of.Doing.Business']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "There are not a lot of missing values in the Starbucks dataset (the adjusted data quality score for missing values is a 100% which means all the featurs that we care about have no missing values, see section on DQS above). For the WorldBank dataset has a lot of missing though, DQS for missing values is ~ 33.65%. The missing values in the WorldBank dataset are handled as follows:\n",
    "1. The WorldBank data is updated every year and many times there is a timelag in the data being available for a year. Also since these indicators are macro level indicators so the year on year change is not drastic. As a first step, all the values that are missing for the latest year are filled with the values from the previous year whenever the previous year's data is applicable.\n",
    "\n",
    "2. Define a feature density threshold of 90% and delete all features which are less than 90% filled.\n",
    "\n",
    "3. Finally the analysis is to be performed on the combined dataset, as described above the combined dataset is obtained by joining the WorldBank and Starbucks datasets only for those countries that exist in the Starbucks dataset. This intersection results in a dataset which is pretty full (99.8%). \n",
    " - Any missing values are handled by replacing them with the mean value of the feature.\n",
    "\n",
    "The DQS metrics for the original datasets as well as the combined dataset after cleaning are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datasource</th>\n",
       "      <th>Combined_Score</th>\n",
       "      <th>Invalid_Data_Raw_Score</th>\n",
       "      <th>Invalid_Data_Adjusted_Score</th>\n",
       "      <th>Missing_Data_Raw_Score</th>\n",
       "      <th>Missing_Data_Adjusted_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WorldBank</td>\n",
       "      <td>76.92776</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>53.855519</td>\n",
       "      <td>53.855519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Starbucks</td>\n",
       "      <td>99.99981</td>\n",
       "      <td>99.99962</td>\n",
       "      <td>99.99962</td>\n",
       "      <td>91.529921</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Datasource  Combined_Score  Invalid_Data_Raw_Score  \\\n",
       "0  WorldBank        76.92776               100.00000   \n",
       "1  Starbucks        99.99981                99.99962   \n",
       "\n",
       "   Invalid_Data_Adjusted_Score  Missing_Data_Raw_Score  \\\n",
       "0                    100.00000               53.855519   \n",
       "1                     99.99962               91.529921   \n",
       "\n",
       "   Missing_Data_Adjusted_Score  \n",
       "0                    53.855519  \n",
       "1                   100.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DQS_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/dqs/dqs_after_cleaning.csv'\n",
    "df = pd.read_csv(DQS_URL)\n",
    "#This is the DQS after Step 1 described above\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>before_cleaing</th>\n",
       "      <th>after_cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WDI+SB</td>\n",
       "      <td>61.20915</td>\n",
       "      <td>99.812312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  before_cleaing   after_cleaning\n",
       "0  WDI+SB        61.20915        99.812312"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DQS_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/dqs/dqs_combined_dataset.csv'\n",
    "df = pd.read_csv(DQS_URL)\n",
    "#This is the DQS after Step 3 described above (Combined dataset, actually used for analysis)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning for the combined dataset\n",
    "The combined dataset is cleaned for missing values as described in the previous section.\n",
    "1. Missing values are handled as described above (take missing data from 2014, remove features for which density is less than 90%, many rows get removed when we combined the WorldBank data with Starbucks data). \n",
    "2. There are no invalid values in the WorldBank dataset (see Data Issues and Data Quality Score sections above). The invalid values from the Starbucks dataset (some lat/long values were missing, names of some streets could not be read due to errors with unicode handling) do not impact the overall analysis and are therefore ignored, see adjusted DQS for invalid data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms and Correlations\n",
    "This section provides histograms and corelations from the individual datasets where appropriate and also from the combined dataset used for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms from WorldBank dataset\n",
    "The following figure shows histogram of three different features from the WorldBank dataset. \n",
    "![](https://github.com/aarora79/sb_study/blob/master/output/EDA/WDI_hist.png?raw=true)\n",
    "\n",
    "The observations from these histograms are described below.\n",
    "\n",
    "| Feature     | Description         |Observations|\n",
    "| ------------- |-------------|---------------| \n",
    "| IS.BUS.EASE.XQ    |  Ease of doing business index (1=most business-friendly regulations)| Almost uniformaly distributed except for the 100 to 150 range (indicating more concentration of countries in that range).|\n",
    "| SP.URB.GROW   | Urban population growth (annual %)|Almost normally distributed.|\n",
    "| WP15163_4.1 | Mobile account (% age 15+) |Almost looks like an EL (Exponential logarithmic distribution).|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms from the combined dataset\n",
    "The following figure shows histogram of three different features from the Combined dataset. The value of the coorelation coefficent is also shown.\n",
    "![](https://github.com/aarora79/sb_study/blob/master/output/EDA/Combined_hist.png?raw=true)\n",
    "\n",
    "The observations from these histograms are described below.\n",
    "\n",
    "| Feature     | Description         |Observations|\n",
    "| ------------- |-------------|---------------| \n",
    "| IT.NET.USER.P2    |  Internet users (per 100 people)| Looks like a skewed left distribution. Most countries with Starbucks stores have a very high percentage of internet users, which makes sense if one visualizes a typical coffee shop.|\n",
    "| Num.Starbucks.Stores   | Number of Starbucks stores.| Most countries (infact 69 out of 73 which have Starbucks) have kess thana 1000 stores, which makes sense but doesnt tell as much. A better representation would be to increase the number of bins in this historgram.|\n",
    "| SP.PO.TOTAL | Total population| The outliers in this chart towards the extreme right probably represent India and China.|\n",
    "|ST.IN.ARVL|International tourism, number of arrivals|Intrestingly, it appears that most Starbucks stores are not located in countries with very high tourist footfals.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots and correlation coefficient WorldBank dataset\n",
    "The following figure shows the scatter plot for three features in the WorldBank dataset.\n",
    "![](https://github.com/aarora79/sb_study/blob/master/output/EDA/WDI_scatter_matrix.png?raw=true)\n",
    "\n",
    "From the scatter plots we can observe that there is a negative correlation between ease of doing business and number of Internet users in a country, but, since the ease of doing business is expressed in a way that lower the number greater the ease so therefore it follows that ease of doing business and number of Internet users are positively correlated.\n",
    "\n",
    "Ease doing business is also positvely correlated to per capita GDP. There also seems to be a positive correlation between number of internet users and per capita GDP.\n",
    "\n",
    "It is important to mention here, the oft repeated phrase **correlation does not imply causation**.\n",
    "\n",
    "The following table shows the correlation coefficient between these features and the value of r corrobrates the observations from the scatter plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WB</td>\n",
       "      <td>IC.BUS.EASE.XQ</td>\n",
       "      <td>SL.GDP.PCAP.EM.KD</td>\n",
       "      <td>-0.557920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WB</td>\n",
       "      <td>IC.BUS.EASE.XQ</td>\n",
       "      <td>IT.NET.USER.P2</td>\n",
       "      <td>-0.831488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WB</td>\n",
       "      <td>SL.GDP.PCAP.EM.KD</td>\n",
       "      <td>IT.NET.USER.P2</td>\n",
       "      <td>0.741431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset           feature1           feature2         r\n",
       "0      WB     IC.BUS.EASE.XQ  SL.GDP.PCAP.EM.KD -0.557920\n",
       "1      WB     IC.BUS.EASE.XQ     IT.NET.USER.P2 -0.831488\n",
       "2      WB  SL.GDP.PCAP.EM.KD     IT.NET.USER.P2  0.741431"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "WB_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/EDA/WDI_pearson_r.csv'\n",
    "df = pd.read_csv(WB_URL)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots and correlation coefficient the combined dataset\n",
    "The following figure shows the scatter plot for three features in the WorldBank dataset.\n",
    "![](https://github.com/aarora79/sb_study/blob/master/output/EDA/Combined_satter_matrix.png?raw=true)\n",
    "\n",
    "From the scatter plots we can observe Number of Starbucks stores has somewhat of a positive non-linear relationship with the number of international tourist arrivals, total population and number of people having access to the Internet. This is further clarified when we do polynomial regression between number of Starbucks stores and number of international tourist arrivals.\n",
    "\n",
    "The following table shows the correlation coefficient between these features and the value of r corrobrates the observations from the scatter plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combined</td>\n",
       "      <td>Num.Starbucks.Stores</td>\n",
       "      <td>IT.NET.USER.P2</td>\n",
       "      <td>0.028446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combined</td>\n",
       "      <td>Num.Starbucks.Stores</td>\n",
       "      <td>ST.INT.ARVL</td>\n",
       "      <td>0.521229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>combined</td>\n",
       "      <td>Num.Starbucks.Stores</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>0.263139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>combined</td>\n",
       "      <td>IT.NET.USER.P2</td>\n",
       "      <td>ST.INT.ARVL</td>\n",
       "      <td>0.087930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combined</td>\n",
       "      <td>IT.NET.USER.P2</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>-0.324875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>combined</td>\n",
       "      <td>ST.INT.ARVL</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>0.311971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset              feature1        feature2         r\n",
       "0  combined  Num.Starbucks.Stores  IT.NET.USER.P2  0.028446\n",
       "1  combined  Num.Starbucks.Stores     ST.INT.ARVL  0.521229\n",
       "2  combined  Num.Starbucks.Stores     SP.POP.TOTL  0.263139\n",
       "3  combined        IT.NET.USER.P2     ST.INT.ARVL  0.087930\n",
       "4  combined        IT.NET.USER.P2     SP.POP.TOTL -0.324875\n",
       "5  combined           ST.INT.ARVL     SP.POP.TOTL  0.311971"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "WB_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/EDA/Combined_r.csv'\n",
    "df = pd.read_csv(WB_URL)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis\n",
    "Cluster analysis was conducted to find patterns within data (from the combined dataset). Recall that the combined dataset is created by filtering out countries which do not have Starbucks presence, this brings a Starbucks specific dimension to this dataset. The presence of Starbucks stores (lets say in terms of number of stores in a country or any other feature derived from the Starbucks dataset) is the dependant variable in this analysis, therefore any patterns we find in the clusters that are created would be examined in that light i.e. do the cluster member show an affinity to a particular value of the dependant variable.\n",
    "\n",
    "Before doing clustering a Principal Component Analysis (PCA) is first conducted to transform the data into 3 dimensions. PCA in 3 dimensions helps because the clusters can the be visualized in 3D space. Three different types of clustering mechanisms are used viz. KMeans, DBSCAN and Hierarchical (Agglomerative) clustering. This is described in detail in the next few sections, lets examine the PCA results first.\n",
    "\n",
    "The following figure shows the result of PCA in 3 dimensions. Note that PCA only helps to spatially distinguish the data, but seeing the clusters without a spectral differentiation is difficult (which is what is provided by the clustering algorthms).\n",
    "![](https://github.com/aarora79/sb_study/blob/master/output/clustering/PCA.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans, DBSCAN and Heirarchical Clustering\n",
    "The following figures show the result of 3 clustering techniques. It appeas that KMeans(k=5) provides the best custering.\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"https://github.com/aarora79/sb_study/blob/master/output/clustering/KMeans.png?raw=true\" alt=\"KMeans\" style=\"width: 400px\"/> </td>\n",
    "    <td> <img src=\"https://github.com/aarora79/sb_study/blob/master/output/clustering/DBSCAN.png?raw=true\" alt=\"DBSCAN\" style=\"width: 400px\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> <img src=\"https://github.com/aarora79/sb_study/blob/master/output/clustering/Hierarchical.png?raw=true\" alt=\"Hierarchichal\" style=\"width: 400px\"/> </td>\n",
    " </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations from clustering\n",
    "1. The plots above indicate that KMeans has the best clusters, although it has to be said that the clusters are not very clear and spaced out even with KMeans which indicates that there may not be very clear patterns in the data. \n",
    "2. Value of k=5 was choosen as it matches the bins that we want to create for classifying number of Starbucks stores as Very Low, Low, Medium, High and Very high.\n",
    "3. Vectors at the center of the clusters (KMeans) are centroids and the points in a cluster are all placed around it.\n",
    "4. The hierarchical and DBSCAN clustering produce similar results as can be observed just by visual inspection of the plots, the KMeans clusters seem to be much more well formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific observation from KMeans\n",
    "The following table shows a filtered list of labels from each of the clustering algorithms attached to each entry in the dataset. The KMeans labels does seem to show a pattern in countries that are grouped together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. KMeans label=4 groups together U.S. and China in a cluster of their own,which should not be a surprise. Ofcourse we can see any number of similarities that could have caused this for example GDP, tourist arrival etc. Also from a Starbucks perspective, these countries represent very high number of Starbucks stores (top 2).\n",
    "\n",
    "2. KMeans label=3 groups together Brazil, Russia and other countries of the erstwhile Russian federation. From a Starbucks perspective these countries represent very similar Starbucks store density (number of stores per 100,000 people).\n",
    "\n",
    "3. KMeans label=2 groups together mostly Asian, African and some South American countries which have low to medium ease of business but medium to high population.\n",
    "\n",
    "Other labels can also be explained in a similar way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>KMeans_labels</th>\n",
       "      <th>SL.GDP.PCAP.EM.KD</th>\n",
       "      <th>Ease.Of.Doing.Business</th>\n",
       "      <th>ST.INT.ARVL.Categorical</th>\n",
       "      <th>SP.POP.TOTL.Categorical</th>\n",
       "      <th>Num.Starbucks.Stores</th>\n",
       "      <th>Starbucks.Store.Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>0</td>\n",
       "      <td>98644.171875</td>\n",
       "      <td>Medium</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>19</td>\n",
       "      <td>0.168354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>2</td>\n",
       "      <td>61845.410156</td>\n",
       "      <td>Medium</td>\n",
       "      <td>VL</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>0.073525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oman</td>\n",
       "      <td>3</td>\n",
       "      <td>85342.617188</td>\n",
       "      <td>Medium</td>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "      <td>12</td>\n",
       "      <td>0.267228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>3</td>\n",
       "      <td>65623.968183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VL</td>\n",
       "      <td>VL</td>\n",
       "      <td>3</td>\n",
       "      <td>2.887697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>0</td>\n",
       "      <td>201747.796875</td>\n",
       "      <td>Medium</td>\n",
       "      <td>VL</td>\n",
       "      <td>VL</td>\n",
       "      <td>2</td>\n",
       "      <td>0.351077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lebanon</td>\n",
       "      <td>1</td>\n",
       "      <td>42497.589844</td>\n",
       "      <td>Low</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>0.495664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El Salvador</td>\n",
       "      <td>1</td>\n",
       "      <td>18404.509766</td>\n",
       "      <td>Medium</td>\n",
       "      <td>VL</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>0.163223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Romania</td>\n",
       "      <td>3</td>\n",
       "      <td>37818.410156</td>\n",
       "      <td>Medium</td>\n",
       "      <td>M</td>\n",
       "      <td>H</td>\n",
       "      <td>25</td>\n",
       "      <td>0.126056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>3</td>\n",
       "      <td>30870.919922</td>\n",
       "      <td>Medium</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>11</td>\n",
       "      <td>0.228792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>1</td>\n",
       "      <td>31734.800781</td>\n",
       "      <td>Low</td>\n",
       "      <td>M</td>\n",
       "      <td>H</td>\n",
       "      <td>105</td>\n",
       "      <td>0.241842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  KMeans_labels  SL.GDP.PCAP.EM.KD  \\\n",
       "0              Belgium              0       98644.171875   \n",
       "1  Trinidad and Tobago              2       61845.410156   \n",
       "2                 Oman              3       85342.617188   \n",
       "3                Aruba              3       65623.968183   \n",
       "4           Luxembourg              0      201747.796875   \n",
       "5              Lebanon              1       42497.589844   \n",
       "6          El Salvador              1       18404.509766   \n",
       "7              Romania              3       37818.410156   \n",
       "8           Costa Rica              3       30870.919922   \n",
       "9            Argentina              1       31734.800781   \n",
       "\n",
       "  Ease.Of.Doing.Business ST.INT.ARVL.Categorical SP.POP.TOTL.Categorical  \\\n",
       "0                 Medium                       M                       M   \n",
       "1                 Medium                      VL                       L   \n",
       "2                 Medium                       L                       L   \n",
       "3                    NaN                      VL                      VL   \n",
       "4                 Medium                      VL                      VL   \n",
       "5                    Low                       L                       M   \n",
       "6                 Medium                      VL                       M   \n",
       "7                 Medium                       M                       H   \n",
       "8                 Medium                       M                       M   \n",
       "9                    Low                       M                       H   \n",
       "\n",
       "   Num.Starbucks.Stores  Starbucks.Store.Density  \n",
       "0                    19                 0.168354  \n",
       "1                     1                 0.073525  \n",
       "2                    12                 0.267228  \n",
       "3                     3                 2.887697  \n",
       "4                     2                 0.351077  \n",
       "5                    29                 0.495664  \n",
       "6                    10                 0.163223  \n",
       "7                    25                 0.126056  \n",
       "8                    11                 0.228792  \n",
       "9                   105                 0.241842  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Combined_W_Labels_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/clustering/WDI_SB_w_labels.csv'\n",
    "df = pd.read_csv(Combined_W_Labels_URL)\n",
    "df[['name', 'KMeans_labels', 'SL.GDP.PCAP.EM.KD','Ease.Of.Doing.Business','ST.INT.ARVL.Categorical','SP.POP.TOTL.Categorical','Num.Starbucks.Stores','Starbucks.Store.Density']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules / Frequent Itemset Mining Analysis\n",
    "Association rule mining is run on the combined dataset to find tuples of various feature values that occur together in the dataset and draw inferences from that. Most of the data in the combined dataset is continous or discrete, to run association rule mining the first step that is needed is to convert data into categorical values. The combined dataset contains 40 odd features, so instead of converting the entire dataset to categorical, some selected features are converted and association rule mining is run for those features. \n",
    "\n",
    "Continous/discrete features are converted into categorical by binning them by percentile values as described in previous sections. The dataset for this particular problem does not lend itself very well for this type of analysis (because large number of features are numeric). The conversion of numeric data to categorical in the context of this dataset is an art rather than a science (meaning what bin sizes are good for categorizing number of stores as low, medium or high, this is subjective) therefore we can get vastly different results by changing the criteria for converting numeric data to categorical (have 3 bins instead of 5 for example). Regression and classification algorithms discussed later are better suited for this data science problem.\n",
    "\n",
    "To select which features would have a relationship with dependant variables that we want to predict we draw scatter plots of the dependant feature Vs all numeric features in the dataset and manually examine where a correlation is seen. Another technique used for finding out the most important feature using a machine learning technique (ExtraForestClassifier) and then using the top two most important features for conversion to categorical. The scatter plots are not being included in this report allthough they are available as part of the overall package that has been submitted. Similarly the list of important features can be seen as part of the logs generated on running the program.\n",
    "\n",
    "Two different types of rule mining is done, one with a rule containing a single variable predicting the dependant variable and another one with two variables predicting the dependant variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules with 2 features \n",
    "The following table identifies all th association rules with two variables:\n",
    "\n",
    "| Feature     | Description         |Categorical values|\n",
    "| ------------- |-------------|---------------| \n",
    "| Num.Starbucks.Stores.Categorical    |  Number of Starbucks stores, categorical| VL, L, M, H, VH|\n",
    "| ST.INT.ARVL.Categorical     |  International tourist arrivals, categorical| VL, L, M, H, VH|\n",
    "\n",
    "The expectation is that very high influx of tourists should occur together with very high number of Starbucks stores. This does occur as this is the second most frequent rule as shown in the table below, however the support value for this rule is a low 0.125 and the confidence is 0.64. The rule that occurs most frequently is medium tourist influx corresponds to medium Starbucks stores, this rule occurs 15 times and has a support of 0.20 and confidence of 0.53. Expected to have a greater support level for the more frequent rules but this not happening probably because of two reasons a) there are only 72 countries with Starbucks stores so there is not a lot data and b) the conversion of numeric to cateogrical data in this case is not an exact science and can be tweaked to give better results (reduce the number of categories for example).\n",
    "\n",
    "This is all summarized in the following table. The rules themselves are written in the following format R:({A})->B means if A is present then B is also present (with the frequency, support and confidence mentioned alongside to give a sense of probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>frequency</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R:({SBS_M})-&gt;TOURIST_ARR_M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>R:({TOURIST_ARR_M})-&gt;SBS_M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>R:({TOURIST_ARR_VH})-&gt;SBS_VH</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>R:({SBS_VH})-&gt;TOURIST_ARR_VH</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>R:({TOURIST_ARR_M})-&gt;SBS_H</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            rule  frequency   support  confidence\n",
       "0     R:({SBS_M})->TOURIST_ARR_M         15  0.208333    0.535714\n",
       "18    R:({TOURIST_ARR_M})->SBS_M         15  0.208333    0.535714\n",
       "29  R:({TOURIST_ARR_VH})->SBS_VH          9  0.125000    0.600000\n",
       "12  R:({SBS_VH})->TOURIST_ARR_VH          9  0.125000    0.600000\n",
       "19    R:({TOURIST_ARR_M})->SBS_H          7  0.097222    0.250000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ASSOC_RULES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/association_rules/association_rules_2_features.csv'\n",
    "df = pd.read_csv(ASSOC_RULES_URL)\n",
    "df.sort_values(by='frequency', ascending=False).head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Association rules with 2 features filtered by 3 different support levels\n",
    "\n",
    "The following tables present association rules filterd by 3 different support levels.\n",
    "1. support >= 0.05 and confidence >= 0.25\n",
    "2. support >= 0.07 and confidence >= 0.25\n",
    "3. support >= 0.1 and confidence >= 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>frequency</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R:({SBS_M})-&gt;TOURIST_ARR_M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R:({SBS_VL})-&gt;TOURIST_ARR_VL</td>\n",
       "      <td>5</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R:({SBS_H})-&gt;TOURIST_ARR_M</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R:({SBS_H})-&gt;TOURIST_ARR_VH</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R:({SBS_VH})-&gt;TOURIST_ARR_VH</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R:({SBS_VH})-&gt;TOURIST_ARR_H</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R:({TOURIST_ARR_M})-&gt;SBS_M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>R:({TOURIST_ARR_M})-&gt;SBS_H</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R:({TOURIST_ARR_VL})-&gt;SBS_VL</td>\n",
       "      <td>5</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>R:({TOURIST_ARR_L})-&gt;SBS_M</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>R:({TOURIST_ARR_VH})-&gt;SBS_VH</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>R:({TOURIST_ARR_VH})-&gt;SBS_H</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>R:({TOURIST_ARR_H})-&gt;SBS_M</td>\n",
       "      <td>6</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>R:({TOURIST_ARR_H})-&gt;SBS_VH</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            rule  frequency   support  confidence\n",
       "0     R:({SBS_M})->TOURIST_ARR_M         15  0.208333    0.535714\n",
       "1   R:({SBS_VL})->TOURIST_ARR_VL          5  0.069444    0.555556\n",
       "2     R:({SBS_H})->TOURIST_ARR_M          7  0.097222    0.500000\n",
       "3    R:({SBS_H})->TOURIST_ARR_VH          4  0.055556    0.285714\n",
       "4   R:({SBS_VH})->TOURIST_ARR_VH          9  0.125000    0.600000\n",
       "5    R:({SBS_VH})->TOURIST_ARR_H          4  0.055556    0.266667\n",
       "6     R:({TOURIST_ARR_M})->SBS_M         15  0.208333    0.535714\n",
       "7     R:({TOURIST_ARR_M})->SBS_H          7  0.097222    0.250000\n",
       "8   R:({TOURIST_ARR_VL})->SBS_VL          5  0.069444    0.625000\n",
       "9     R:({TOURIST_ARR_L})->SBS_M          4  0.055556    0.571429\n",
       "10  R:({TOURIST_ARR_VH})->SBS_VH          9  0.125000    0.600000\n",
       "11   R:({TOURIST_ARR_VH})->SBS_H          4  0.055556    0.266667\n",
       "12    R:({TOURIST_ARR_H})->SBS_M          6  0.083333    0.428571\n",
       "13   R:({TOURIST_ARR_H})->SBS_VH          4  0.055556    0.285714"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ASSOC_RULES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/association_rules/association_rules_2_features_min_support_0.05_and_min_confidence_0.25.csv'\n",
    "df1 = pd.read_csv(ASSOC_RULES_URL)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>frequency</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R:({SBS_M})-&gt;TOURIST_ARR_M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R:({SBS_H})-&gt;TOURIST_ARR_M</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R:({SBS_VH})-&gt;TOURIST_ARR_VH</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R:({TOURIST_ARR_M})-&gt;SBS_M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R:({TOURIST_ARR_M})-&gt;SBS_H</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R:({TOURIST_ARR_VH})-&gt;SBS_VH</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R:({TOURIST_ARR_H})-&gt;SBS_M</td>\n",
       "      <td>6</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           rule  frequency   support  confidence\n",
       "0    R:({SBS_M})->TOURIST_ARR_M         15  0.208333    0.535714\n",
       "1    R:({SBS_H})->TOURIST_ARR_M          7  0.097222    0.500000\n",
       "2  R:({SBS_VH})->TOURIST_ARR_VH          9  0.125000    0.600000\n",
       "3    R:({TOURIST_ARR_M})->SBS_M         15  0.208333    0.535714\n",
       "4    R:({TOURIST_ARR_M})->SBS_H          7  0.097222    0.250000\n",
       "5  R:({TOURIST_ARR_VH})->SBS_VH          9  0.125000    0.600000\n",
       "6    R:({TOURIST_ARR_H})->SBS_M          6  0.083333    0.428571"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASSOC_RULES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/association_rules/association_rules_2_features_min_support_0.07_and_min_confidence_0.25.csv'\n",
    "df2 = pd.read_csv(ASSOC_RULES_URL)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>frequency</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R:({SBS_M})-&gt;TOURIST_ARR_M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R:({SBS_VH})-&gt;TOURIST_ARR_VH</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R:({TOURIST_ARR_M})-&gt;SBS_M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R:({TOURIST_ARR_VH})-&gt;SBS_VH</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           rule  frequency   support  confidence\n",
       "0    R:({SBS_M})->TOURIST_ARR_M         15  0.208333    0.535714\n",
       "1  R:({SBS_VH})->TOURIST_ARR_VH          9  0.125000    0.600000\n",
       "2    R:({TOURIST_ARR_M})->SBS_M         15  0.208333    0.535714\n",
       "3  R:({TOURIST_ARR_VH})->SBS_VH          9  0.125000    0.600000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASSOC_RULES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/association_rules/association_rules_2_features_min_support_0.1_and_min_confidence_0.25.csv'\n",
    "df = pd.read_csv(ASSOC_RULES_URL)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules with 3 features \n",
    "The following table identifies all th association rules with two variables:\n",
    "\n",
    "| Feature    | Description         |Categorical values|\n",
    "| ------------- |-------------|---------------| \n",
    "| Num.Starbucks.Stores.Categorical    |  Number of Starbucks stores, categorical| VL, L, M, H, VH|\n",
    "| ST.INT.ARVL.Categorical     |  International tourist arrivals, categorical| VL, L, M, H, VH|\n",
    "|SP.POP.TOTL.Categorical | Total population, categorical| VL, L, M, H, VH|\n",
    "\n",
    "The expectation is that high influx of tourists and a high local population should occur together with high number of Starbucks stores. This does occur as this is the second most frequent rule as shown in the table below, however the support value for this rule is a very low 0.08 but the confidence is 0.75. The rule that occurs most frequently is medium tourist influx in a country with medium population corresponds to medium Starbucks stores, this rule occurs 7 times and has a support of 0.09 and confidence of 0.53. Expected to have a greater support level for the more frequent rules but this not happening probably because of two reasons a) there are only 72 countries with Starbucks stores so there is not a lot data and b) the conversion of numeric to cateogrical data in this case is not an exact science and can be tweaked to give better results (reduce the number of categories for example).\n",
    "\n",
    "This is all summarized in the following table. The rules themselves are written in the following format R:({A,B})->C means if A and B present together does imply C (with the frequency, support and confidence mentioned alongside to give a sense of probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>frequency</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R:({POPT_M,TOURIST_ARR_M})-&gt;SBS_M</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>R:({POPT_VH,TOURIST_ARR_VH})-&gt;SBS_VH</td>\n",
       "      <td>6</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>R:({POPT_M,TOURIST_ARR_H})-&gt;SBS_M</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>R:({POPT_VL,TOURIST_ARR_VL})-&gt;SBS_VL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R:({POPT_M,TOURIST_ARR_M})-&gt;SBS_H</td>\n",
       "      <td>3</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    rule  frequency   support  confidence\n",
       "0      R:({POPT_M,TOURIST_ARR_M})->SBS_M          7  0.097222    0.538462\n",
       "35  R:({POPT_VH,TOURIST_ARR_VH})->SBS_VH          6  0.083333    0.750000\n",
       "9      R:({POPT_M,TOURIST_ARR_H})->SBS_M          4  0.055556    0.500000\n",
       "20  R:({POPT_VL,TOURIST_ARR_VL})->SBS_VL          4  0.055556    0.800000\n",
       "1      R:({POPT_M,TOURIST_ARR_M})->SBS_H          3  0.041667    0.230769"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ASSOC_RULES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/association_rules/association_rules_3_features.csv'\n",
    "df = pd.read_csv(ASSOC_RULES_URL)\n",
    "df.sort_values(by='frequency', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association rules with 3 features filtered by 3 different support levels\n",
    "The following tables present association rules filterd by 3 different support levels.\n",
    "1. support >= 0.05 and confidence >= 0.25\n",
    "2. support >= 0.07 and confidence >= 0.25\n",
    "3. support >= 0.1 and confidence >= 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>frequency</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R:({POPT_M,TOURIST_ARR_M})-&gt;SBS_M</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R:({POPT_M,TOURIST_ARR_H})-&gt;SBS_M</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R:({POPT_VL,TOURIST_ARR_VL})-&gt;SBS_VL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R:({POPT_VH,TOURIST_ARR_VH})-&gt;SBS_VH</td>\n",
       "      <td>6</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   rule  frequency   support  confidence\n",
       "0     R:({POPT_M,TOURIST_ARR_M})->SBS_M          7  0.097222    0.538462\n",
       "1     R:({POPT_M,TOURIST_ARR_H})->SBS_M          4  0.055556    0.500000\n",
       "2  R:({POPT_VL,TOURIST_ARR_VL})->SBS_VL          4  0.055556    0.800000\n",
       "3  R:({POPT_VH,TOURIST_ARR_VH})->SBS_VH          6  0.083333    0.750000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ASSOC_RULES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/association_rules/association_rules_3_features_min_support_0.05_and_min_confidence_0.25.csv'\n",
    "df1 = pd.read_csv(ASSOC_RULES_URL)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>frequency</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R:({POPT_M,TOURIST_ARR_M})-&gt;SBS_M</td>\n",
       "      <td>7</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R:({POPT_VH,TOURIST_ARR_VH})-&gt;SBS_VH</td>\n",
       "      <td>6</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   rule  frequency   support  confidence\n",
       "0     R:({POPT_M,TOURIST_ARR_M})->SBS_M          7  0.097222    0.538462\n",
       "1  R:({POPT_VH,TOURIST_ARR_VH})->SBS_VH          6  0.083333    0.750000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ASSOC_RULES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/association_rules/association_rules_3_features_min_support_0.07_and_min_confidence_0.25.csv'\n",
    "df1 = pd.read_csv(ASSOC_RULES_URL)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>frequency</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [rule, frequency, support, confidence]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ASSOC_RULES_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/association_rules/association_rules_3_features_min_support_0.1_and_min_confidence_0.25.csv'\n",
    "df1 = pd.read_csv(ASSOC_RULES_URL)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analysis\n",
    "This section describes three hypothesis that have been developed using the combined WorldBank and Starbucks dataset and then tests each of the three hypothesis using parameteric and/or predictive algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1: No difference in the average number of Starbucks stores in countries with very high number of international tourists Vs rest of the world.\n",
    "\n",
    "This hypothesis is tested in two ways. \n",
    "1. Two sample T-test: A two sample T-test is used to find out if the means of two independant samples are different. The null hypothesis is that means of both the samples are the same. The two sample t-test only relies on sample means and does not need to know the population mean.\n",
    "\n",
    "2. Linear and Polynomial regression: ordinary least squares regression. Linear regression tries to predict the value of the dependant variable by using a linear combination of explanatory variables. Linear regression could be univariate (one explanatory variable) or multi variate. A polynomial regression tries to model the dependant variable using higher powers of the explanatory variable(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the two sample T-test for hypothesis 1\n",
    "The combined dataset is used to filter out entries belonging to 'ST.INT.ARVL.Categorical' equal to 'VH' (for very high) and then another set of entries corresponding to 'ST.INT.ARVL.Categorical' value other than 'VH' so as to include the rest of the world. Python scipy module is used to calculate a T statistic and a corresponding p-value when comparing the two distributions. The p-value comes out to be ~0.17. This is a unusually large value and with a typical T-test alpha value of 0.05 it would not be possible to reject the null hypothesis but in this case since the distribution of the number of stores was assumed to be normal when it is known that it is not normal (see histogram in previous sections) so a higher alpha of 0.20 is choosen and this enables the null hypothesis that there is no difference to be rejected. This is in line with what is seen in the scatter plots for these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hypothesis</th>\n",
       "      <th>No difference between average number of Starbucks stores in countries with Very high and high number of international tourist Vs Rest of the world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T-statistic, p-value</td>\n",
       "      <td>1.452938,0.168245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Hypothesis  \\\n",
       "0  T-statistic, p-value   \n",
       "\n",
       "  No difference between average number of Starbucks stores in countries with Very high and high number of international tourist Vs Rest of the world  \n",
       "0                                  1.452938,0.168245                                                                                                  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "T_TEST_URL = 'https://raw.githubusercontent.com/aarora79/sb_study/master/output/regression/t_test_results.csv'\n",
    "df1 = pd.read_csv(T_TEST_URL)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Linear and Polynomial regression for hypothesis 1\n",
    "In this method instead of using the categorical value of the number of international tourist arrivals the actual discrete value is used and it provided to a linear regression model and a polynmial regression model to predict the value of the number of Starbucks stores. The linear model provides an explained variance of 0.27 whereas the polynomial model performs much better by providing an exaplined variance of 0.33. Ideally the the explained variance should be close to 1. It is a matter of further investigation as to what else can be done to make the prediction better.\n",
    "The following scatter plots show linear and polynomial regression models in action, as can be seen from the plots the polynomial model provides a better fit. The polynomial model is a degree 3 polynomial (other degrees were also tried and 3 provided the best results). The outlier value seen towards the stop right is the number of stores in te U.S., it is an extreme value and further investigation would need to be done to figure out if a single variable can predict such a huge jump in value.\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"https://github.com/aarora79/sb_study/blob/master/output/regression/linear.png?raw=true\" alt=\"Linear\" style=\"width: 400px\"/> </td>\n",
    "    <td> <img src=\"https://github.com/aarora79/sb_study/blob/master/output/regression/polynomial.png?raw=true\" alt=\"Polynomial\" style=\"width: 400px\"/> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 2: The number of Startbucks stores in a country as a categorical value can be predicted using some WDI indicators\n",
    "\n",
    "This hypothesis states that the number of Starbucks stores in a country expressed as a categorical value (Very High, High, Medium, Low, Very Low) can be predicted using WDI (World Development Indictors) from the WorldBank data. This is verified by running various classification algorithms and then selecting the on that provides the highest accuracy value as measured via cross validation.\n",
    "\n",
    "The combined dataset contains 35 WDI features. All of these are not used for classification, the most important 15 features are choosen. The Extra Trees classifier which uses an ensemble of decision trees is used to provide a ranking for all the features and then the top ranked 15 features are used as inputs for the classification algorithms. Tests were done with 5, 10, 15, 30 and all 35 features as well, 15 most important features provided the best results in terms of accuracy.\n",
    "\n",
    "The 15 most important features for determining the number of stores value as a categorical are:\n",
    "\n",
    "| Feature     | Description         |\n",
    "| ------------- |-------------|\n",
    "|TX.VAL.TECH.CD|High-technology exports (current US dollars)|\n",
    "|ST.INT.ARVL    |  International tourism, number of arrivals|\n",
    "|SP.POP.TOTL|Total Population|\n",
    "|BG.GSR.NFSV.GD.ZS|Trade in services (percentage of GDP)|\n",
    "|SL.GDP.PCAP.EM.KD|GDP per person employed (constant 2011 PPP dollars)|\n",
    "|IC.LGL.CRED.XQ|Strength of legal rights index (0=weak to 12=strong)|\n",
    "|BX.KLT.DINV.WD.GD.ZS | Foreign direct investment, net inflows (percentage of GDP)|\n",
    "|IC.EXP.COST.CD|Cost to export (US$ per container)|\n",
    "|NE.CON.PETC.ZS|Household final consumption expenditure, etc. (percentage of GDP)|\n",
    "|IC.REG.DURS|Time required to start a business (days)|\n",
    "|TX.VAL.OTHR.ZS.WT|Computer, communications and other services (percentage of commercial service exports)|\n",
    "|IC.REG.PROC|Start-up procedures to register a business (number)|\n",
    "|IC.BUS.EASE.XQ| Ease of doing business|\n",
    "|SH.STA.ACSN.UR|Improved sanitation facilities, urban (percentage of urban population with access)|\n",
    "|IT.NET.USER.P2|Internet users (per 100 people)|\n",
    "\n",
    "The above list is a very intresting list because all of these indicators seem to appear as very important factors that a business would consider. A lot of these are either economic or directly related to business activity so it intutively makes sense that the machine learning algorithm was choosing the right indicators.\n",
    "\n",
    "Once the indicators were selected all of the following classification algorithms were applied on the data and the results in terms of accuracy and standard deviation of a K-fold cross validation was noted. Random forest turned out to be the best classifier in terms of accuracy.\n",
    "\n",
    "The accuracy turned out to be in the 50 to 60 percent range but this is just the first pass at predicting. The indicators, how they are used, and finally the bins for the classification of the dependant variable, all are subject to be refined and this is not the final score. The hypothesis is considered valid although the accuracy value is lower than desirable. More investigation needs to be performed to bring the accuracy score up around the 80 percent range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of classification algorithms used\n",
    "All of the following algorithms were used for classification. All of these algorithms are available via the Scikit learn python package. A test train split was done using training data fraction as 20%. A 10-fold cross validation was done to determine the accuracy of various algorithms.\n",
    "\n",
    "| Algorithm   | Description         | Important notes | Results (Accuracy (standard deviation)|\n",
    "| ------------- |-------------|----------------------|---------|\n",
    "|kNN|K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions).| Used scikit learn's defaults. | 0.476667 (0.164688)|\n",
    "|CART|Classification and Regression Trees. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The top most node is called root and is also the best predictor.| Used scikit learn's defaults. | 0.383333 (0.188709)|\n",
    "|Naive Bayes|The Naive Bayesian classifier is based on Bayes’ theorem with independence assumptions between predictors. A Naive Bayesian model is easy to build, with no complicated iterative parameter estimation which makes it particularly useful for very large datasets. |  Used scikit learn's defaults. Does performn well alongiwth kNN and RandomForest for the combined dataset.|0.493333 (0.221008)|\n",
    "|SVM| A Support Vector Machine (SVM) performs classification by finding the hyperplane that maximizes the margin between the two classes. The vectors (cases) that define the hyperplane are the support vectors.| Used scikit learn's defaults (rbf kernel is the default, tried with other kernels all give the same result). Does not perform well for the dataset.|0.363333 (0.210000)|\n",
    "|RandomForest| A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).| Used scikit learn defaults. Provides the best results.|0.510000 (0.201136)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3: The number of Starbucks stores (discrete) in a country can be predicted using some WDI indicators.\n",
    "This hypothesis stats that number of Starbucks stores in a country which is a discrete variable (as opposed to the categorical considered in hypothesis 2) can be predicted using some WDI indicators. The method used here is similar to hypothesis 2 in that first a set of three most important features are determined using the Extra Trees classifier and then these three features are fed into a __Multivariate Polynomial Regression__ model to predict the dependant variable i.e. number of Starbucks store in a country. The three features used for this prediction are  'ST.INT.ARVL' (international tourist arrivals), 'TX.VAL.TECH.CD' (High-technology exports (current US dollars)), 'SP.POP.TOTL' (population of the country).\n",
    "\n",
    "A multivariate polynomial model is choosen since there are more than one explanatory variables and the relationship with the dependant variable is definitely not linear as is known from the scatter plots (not included here but available as part of the package). A degree 3 polynomial works best as determined by experimenting with other values as well. An explained variance score of 0.99 is achieved, however the mean squared error is still very high at 27063. The MSE needs to be improved before the hypothesis can be considered valid, this needs further investigation in terms of feature selection and model used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Hypothesis\n",
    "A couple of other hypothesis were also tested as part of this phase. \n",
    "1. Whether ownership type of Starbucks store ina  country should be mixed (Starbucks owned, franchisee owned or other) or just Starbucks owned. All classification models were tried and it was found that  Random Forest and kNN both predict this with about 84% accuracy (stddev 12%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together\n",
    "At this time in the data science lifecycle a clean combined dataset exists and has been used to validate multiple hypothesis as described above. The focus of this phase was getting everything in place to run predictive algorithms, the next phase will focus on improving the accuracy of the predictions. The results of the predictions are included in the overall package (see regression folder) but are not being disucssed here at this time. \n",
    "\n",
    "What does appear is that more than a regression problem the problem can be modeled as more of a clustering and classification problem. In other words, use clustering algorithms (like KMeans as described above) to see if countries which have no Starbucks stores appear in which clusters, for example if they appear in clusters corresponding say countries with a high number of Starbucks store then that is a prediction. Similarly, the RandomForest or NaiveBayes classifier (which provided about ~60% accuracy) could be used to predict the category (low, medium, high etc) for number of Starbucks stores. \n",
    "\n",
    "Another aspect to review is that maybe the categories for the number of Starbucks stores need to be reduced from 5 (Very high, high, medium, low, very low) to just 3 (high, medium, low) to improve the accuracy of predictions. These are all things to be considered for the next phase of the project."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "attach-environment": true,
   "environment": "py3k",
   "summary": "Relationship between existence of Starbucks stores and economic and human development indices of countries ",
   "url": "https://anaconda.org/aa1603/a-data-science-tale-about-a-coffee-company"
  },
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
